---
layout: post
title: Основы Numpy, Scipy, Pandas
comments: False
category: python
tags:
---

# Numpy

```python
import numpy as np
x = np.array([1, 2, 4]) # простой массив
x
>>> array([1, 2, 4])
x.dtype # какие данные хранятся в массиве
>>> dtype('int64')
```

```python
x = np.array([1, 2, 4], dtype='float32') # задали тип данных
x
>>> array([1., 2., 4.], dtype=float32)
x.shape # узнать размерность массива
>>> (3,)
```

Атрибут **shape** возвращает набор размерности по всем осям,
которые у нас есть в массиве, то есть если бы это был какой-то двумерный массив, у нас бы вернулась пара число строк и число столбцов, но в данном случае у нас получается одномерный массив и возвращается только одно значение. 

**Двумерная матрица**

```python
m = np.array([[1, 2, 4], [3, 2, 1]]) # передаем лист с литами
m
>>> array([[1, 2, 4],
>>>       [3, 2, 1]])
m.shape
>>> (2, 3)
```

Numpy массивы можно создавать несколькими способами. Прежде был лист на основе листа.


Также можно создать массивы на основе встроенной функции.

```python
# единичная
np.ones(3)
>>> array([1., 1., 1.])

# нулевая
np.zeros(3)
>>> array([0., 0., 0.])

# диагональная
np.eye(4)
>>> array([[1., 0., 0., 0.],
>>>       [0., 1., 0., 0.],
>>>       [0., 0., 1., 0.],
>>>       [0., 0., 0., 1.]])

# со случайными числами
np.random.random([2,3])
>>> array([[0.31382662, 0.23470768, 0.40722605],
>>>       [0.0262462 , 0.49801116, 0.58328842]])
```

## Взаимодействие с массивом

**Как работает индексация в numpy**

Если же у нас какой-то одномерный массив, мы можем просто выбрав нужный нам индекс, вытащить значения, и соответственно, так же как с листом, индексация начинается с нуля

```python
x = np.array([1, 2, 4])
x[2]
>>> 4
```

В двумерной матрице

```python
y = np.array([[1, 2, 3],[4, 5, 6]])
# первый в квадратных скобках индекс по оси Х
# второй по Y
y[0,1]

## Промежуток индексов ##
# первое строки, второе столбцы
y[:, :2] 
>>> array([[1, 2],
>>>       [4, 5]])

## Значения по условию ##
# маска
y = np.array([[1, 2, 3],[4, 5, 6]])
y > 2
>>> array([[False, False,  True],
>>>       [ True,  True,  True]])

# маску передаем как индекс
y[y>2]
>>> array([3, 4, 5, 6])
```

### изменение размерности матрицы

При работе с многомерными массивами, у нас часто возникает ситуация, что нам необходимо как-то поменять форму, либо размерность данных, например, если у нас была какая-то многомерная структура, мы хотим ее развернуть в один массив, одномерный, и посмотреть какие-то значения. В numpy это можно осуществить, используя следующие методы: **flatten** и **reshape**. 

```python
b = np.array([[3, 1, 2, 5], [6, 8, 9, 7]])

b.shape
>>> (2, 4)

b.flatten()
>>> array([3, 1, 2, 5, 6, 8, 9, 7])

b.T
>>> array([[3, 6],
>>>       [1, 8],
>>>       [2, 9],
>>>       [5, 7]])

b.reshape((4, 2)) # значения по осям
>>> array([[3, 1],
>>>       [2, 5],
>>>       [6, 8],
>>>       [9, 7]])
```

Используя метод **resize**,
можно поменять форму нашего исходного массива.
Отличие **resize** от **reshape** заключается в том,
что **resize** автоматически меняет
исходный массив в то время как **reshape** просто изменяет ее форму. 

```python
b.resize(4,2)
b
>>> array([[3, 1],
>>>       [2, 5],
>>>       [6, 8],
>>>       [9, 7]])
```

**Математические операции**

```python
v = np.array([9, 10])
w = np.array([22, 10])

v+w
>>> array([31, 20])

v*w
>>> array([198, 100])

np.dot(v, w) # скалярное произведение
>>> 298
```

# Scipy

Библиотека **scipy** очень удобная библиотека для различного рода вычислений. Она включает в себя методы оптимизации, методы линейной алгебры, обработки сигналов и изображений. 

## Нахождение детерминанта матрицы, обратной матрицы, собственные числа и вектора

```python
from scipy import linalg
from scipy import optimize
import numpy as np
import matplotlib.pyplot as plt

A = np.array([[1, 3, 5],[2, 5, 1],[2, 3, 8]])
linalg.det(A)
>>> -25.000000000000004
# Детерминант не равен нулю
# найдем обратную матрицу

linalg.inv(A)
>>> array([[-1.48,  0.36,  0.88],
>>>        [ 0.56,  0.08, -0.36],
>>>        [ 0.16, -0.12,  0.04]])

# метод возвращает пару, где 1-собственные числа, 2-собственные вектора
eigenvalues, eigenvectors = linalg.eig(A)
eigenvalues
>>> array([10.5540456 +0.j, -0.5873064 +0.j,  4.03326081+0.j])

eigenvectors
>>> array([[-0.51686204, -0.94195144,  0.11527992],
>>>        [-0.32845853,  0.31778071, -0.81936883],
>>>        [-0.79054957,  0.10836468,  0.56155611]])
```

## Оптимизация

```python
def f(x):
    return x**2 + 6*np.sin(x)
x = np.arange(-10,10,0.1)
plt.plot(x, f(x))
plt.show()
```

<img src="/assets/img/2020-12-25-python-dlya-analiza-dannyh/1.png">

Если мы посмотрим на график этой функции, то мы увидим, что у неё есть локальный минимум и глобальный. Глобальный минимум находится в районе **−1**. Давайте попробуем его вычислить. Для вычисления минимума функции в **scipy** есть соответствующий метод **minimize**. Давайте его вызовем и передадим в качестве аргумента нашу функцию, а также начальное приближение, то есть та область, с которой мы начнем поиск минимума. Давайте зададим его равным **0**. Увидим, что результатом работы функции будет некоторый объект, в котором присутствует множество параметров. 

```python
optimize.minimize(f, x0=0)
```

<img src="/assets/img/2020-12-25-python-dlya-analiza-dannyh/2.png">

Многие из этих параметров относятся к выбранному методу оптимизации, а **последний будет соответствовать минимуму функции. Увидим, что метод вернул в качестве минимума значение −1, что соответствует глобальному минимуму.** 

Ещё раз глянем на график и посмотрим, в каких случаях метод **minimize** может ошибаться. Так как у нас есть локальный минимум, то случайно вместо глобального минимума метод может вернуть локальный минимум, и этого легко добиться, изменив только один параметр, например, начальное приближение. Давайте зададим значение, не равное **0**, например, возьмём значение, равное **3**. И мы видим, что результат, который соответствует минимуму, в данном случае близится к значению **3**, что похоже на один из локальных минимумов функции.

```python
optimize.minimize(f, x0=3)
>>> x: array([2.93810277])
```

# Pandas

Библиотека поддерживает загрузку текстовых файлов, бинарных, а также можно подключаться к базам данных и работать с ними напрямую. 

Pandas оперируют двумя основными структурами данных: Series и DataFrame.

Series - индексированный массив некоторого типа: числовой, бинарный или категориальный.
DataFrame - двумерная структура данных, совокупность Series. 

## Объект pandas.Series


Структура Series представляет из себя объект, похожий на одномерный массив, но отличительной его чертой является наличие меток, то есть индексов вдоль каждого элемента из списка. 

```python
s = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s

>>> a    1
>>> b    2
>>> c    3
>>> d    4
>>> dtype: int64
```

```python
s = pd.Series([1, 2, 3, 4])
s

>>> 0    1
>>> 1    2
>>> 2    3
>>> 3    4
>>> dtype: int64
```

Также Series можно создавать, передавая не только лист с данными, но и **словарь, ключи которого станут индексами** в созданной Series. 

```python
d = {"Minsk": 1000, "Grodno": 500, "Brest": 300}
k = pd.Series(d)
k

>>> Minsk     1000
>>> Grodno     500
>>> Brest      300
>>> dtype: int64

k['Minsk']
>>> 1000

k[['Minsk', 'Brest']]
>>> Minsk    1000
>>> Brest     300
>>> dtype: int64
```

Если же мы хотим как-то отфильтровать или выбрать строки по некоторым индексам, то нам достаточно передать список этих индексов, и нам вернутся соответствующие значения. 

Вытащить из Series вытащить некоторые строки, которые удовлетворяют некоторым условиям, например, значения не превышают заданного предела. Для этого для начала нам необходимо создать маску, то есть написать то условие, которому должна соответствовать Series, а потом передать его в качестве индекса. 

```python
k
k[k>400]

>>> Minsk     1000
>>> Grodno     500
>>> dtype: int64
```

**Как поменять значение Series?** Для этого достаточно выбрать значение в нужном нам индексе и присвоить ему новое. 

```python
k['Minsk'] = 100
```

Можно поменять и по маске

```python
k[k<500] = 199
```

**Как отфильтровать NaN строки**

В Pandas реализовано два метода — **isnull** и **notnull**.

```python
k[k.isnull()]
```

## Объект pandas.DataFrame

```python
df = pd.read_csv('citibike.csv')
df.head(3) # первые 3 строки
df.tail(3) # последние 3 строки
df.shape # размер ДФ. (<число строк>,<число столбцов>)
df.columns # получить только названия столбцов
df.dtypes # типы данных в ДФ
```

**Вывести только нужные столбцы**

```python
df[['starttime', 'endtime']].head()
```

**Как обращаться к элементам ДФ**

**loc/iloc**

**iloc** позволяет по индексу обращаться к строкам и столбцам. Допустим, если нам необходимо вытащить значения хранящиеся в самой последней строке, мы можем в ilоc указать индекс минус 1 и по умолчанию вернется последняя строка. А также с помощью метода iloc можно посмотреть какое значение будет лежать в DataFrame на пересечении **столбца и строки**. 

```python
df.iloc[-1]
df.iloc[-1, 4]
>>> 'E 2 St & Avenue B'
```

**А также есть другой метод, который называется loc.**

Он также работает, мы указываем значение по строке и указываем уже индекс по столбцу, но он отличается от iloc тем, что мы можем указывать значение слайсы по лейблу. 

```python
df.loc[1,['tripduration']]
>>> tripduration    690
>>> Name: 1, dtype: object
```

*Эти 2 метода имеют некоторые отличия.*

Например, если мы хотим отфильтровать с помощью метода iloc указывая вот эти границы, то у нас **не включается крайние значения**, то есть мы видим, что у нас возвращаются строки от 0 до 5, не включая ту строку с индексом 6, и аналогично по столбцам.

Метод же **loc работает наоборот, он включает крайние значения**, и мы здесь видим, что возвращаемый DataFrame на одну строку и один столбец больше, чем в предыдущем случае. 

<img src="/assets/img/2020-12-25-python-dlya-analiza-dannyh/3.png">

**Как выбрать строки, удовлетворяющие условию**

Хотим выбрать только те поездки, вина которых меньше, чем 400 секунд. Как это можно сделать? Нам **нужно создать маску, которая будет являться нашим условием и передать ее в качестве индекса нашему DataFrame**. 

```python
df[df['tripduration']<400].head()
```

**Можно создавать несколько условий на фильтрацию**

Мы можем отобрать только те поездки, длина которых меньше, чем 1000 секунд и выбрать определенный тип пользователя, и эти два условия можно связать логическим оператором. 

```python
df[(df['tripduration']<400) & (df['usertype']=='Subscriber')].shape
```

**Статистики по DataFrame**

```python
df.describe()
```

Так как наш **DataFrame** хранит не только числовые, но и категориальные признаки, то с помощью метода "**describe**" по ним тоже можно посмотреть статистику, но нужно указать, какой тип данных мы хотим увидеть. И в данном случае у нас возвращается статистика по категориальным признакам, увидим, что здесь присутствует такая статистика, как общее число встречаемости и количество уникальных значений. 

```python
df.describe(include=[np.object])
```

<img src="/assets/img/2020-12-25-python-dlya-analiza-dannyh/4.png">

**Статистика по значениям внутри столбца**

```python
df['usertype'].value_counts(normalize=True)

>>> Subscriber    0.874169
>>> Customer      0.125831
>>> Name: usertype, dtype: float64
```

```python
df['gender'].unique()
```

**Корреляция между всеми столбцами**

```python
df.corr()
```

**Как сделать сэмпл DataFrame**

```python
df.sample(frac=0.1)
```

**Как сохранить**

```python
df.to_scv('path_to_file.csv')
```


## Группировка данных

Как можно группировать данные в pandas, и какие можно делать манипуляции с получившими группами. 

```python
df.groupby(['usertype']).groups
```

Для этого нам необходимо вызвать метод groupby и передать тот столбец,
по которому мы хотим сгруппировать наши строки. Например, это тип пользователей. Мы видим, что возвращается некоторый объект датафрейма groupby, и чтобы посмотреть, какие у нас получились группы, можно воспользоваться атрибутом groups, в результате которого у нас вернется некоторый словарь, где в качестве ключа лежат значения с группируемой переменной, в данном случае их всего два, а в качестве значения лежат индексы строк, в которых встречается именно это значение. Это такое представление групп. А затем мы можем помимо такого представления в виде индексов уже посмотреть какие конкретно значения и строки у нас встречаются в группе. 

```python
df.groupby(['usertype']).first()
```

<img src="/assets/img/2020-12-25-python-dlya-analiza-dannyh/5.png">

И теперь, когда у нас уже есть некоторая сгруппированная структура, мы можем посчитать некоторое распределение значений. Допустим, мы хотим **посмотреть среднюю продолжительность поездок по каждой группе пользователей**. 

```python
df.groupby(['usertype'])[['tripduration']].mean()
```

<img src="/assets/img/2020-12-25-python-dlya-analiza-dannyh/6.png">

Осуществляем группировку данных, а затем, после groupby мы указываем лист значений необходимых для агрегирования, и затем метод, который мы хотим, по которому мы собственно говоря агрегируем данные, например, средняя продолжительность поездок. У нас возвращается некоторый датафрейм, а также, если мы укажем в качестве аргумента не лист значений, а всего лишь одно значение, в данном случае продолжительность поездки, у нас уже вернется объект типа series.

Также позволяет группировать данные не только по какому-то одному признаку, а сразу же по группе признаков.

Для этого нам просто необходимо добавить еще дополнительный столбец в метод groupby. 

```python
df.groupby(['usertype', 'start station name'])[['tripduration']].mean()
```

Если интересует какое-либо распределение значения не только по одному признаку, а сразу же по нескольким признакам, то это можно сделать, используя метод **aggregate**, то есть сначала мы также группируем наши данные, затем мы вызываем метод **aggregate**, где в качестве аргумента мы можем передать словарь следующего вида: мы передаем в качестве ключей нужные признаки для агрегации, и мы можем передать, например, сразу же несколько признаков. В результате применения метода **aggregate** возвращается датафрейм, где индекс - это наш исходный признак группировки и соответствующие два столбца с нужными значениями. Также, если нам необходимо посмотреть изменение значений или вообще какие-либо другие метрики по какому-то одному признаку, мы можем в этом же словаре, для этого признака, в качестве значения указывать лист методов, и в результате получившийся датафрейм мы видим, что по признаку, в котором указано сразу несколько методов, возвращаются уже другие значения. 

```python
df.groupby(['usertype']).agg({'tripduration': sum, 'starttime':'first'})
```

<img src="/assets/img/2020-12-25-python-dlya-analiza-dannyh/7.png">

```python
df.groupby(['usertype']).agg({'tripduration': [sum, min], 'starttime':'first'})
```

<img src="/assets/img/2020-12-25-python-dlya-analiza-dannyh/8.png">

Вариант, когда мы хотим посчитать какой-нибудь значение, не используя встроенные функции, а используя какие-то свои функции. Это можно сделать, например, **используя лямбда функцию**, и соответственно, значение будет меняться. 

```python
df.groupby(['usertype']).agg({'tripduration': lambda x: max(x) + 1,
                              'starttime':'first'})
```

## Работа с несколькими таблицами

**В Pandas существует два метода join'а таблиц.**

Один называется **merge**, другой — **join**.
Они отличаются тем, что с помощью метода **merge** мы можем связывать две таблицы по каким-то признакам, а в случае **join** мы связываем две таблицы по какому-то общему индексу. 

```python
pd.merge(listings_df, reviews_df,
    left_on=['id'], right_on=['listing_id'],
    how='inner', indicator=True)
```

По умолчанию метод merge, если не указывать, какой метод **join** мы хотим осуществить, осуществляет **left join**, то есть добавляет все значения из правой таблицы к левой. Также мы можем это поменять, просто указав в аргументе нужный метод. 

```python
calendar_df.set_index('listing_id', inplace=True)
reviews_df.set_index('listing_id', inplace=True)
calendar_df.join(reviews_df, lsuffix='listing_id', rsuffix='listing_id')
```

## Преобразование признаков

Методы **map** и **apply**.

Давайте начнем с функции **map** С помощью функции **map** можно преобразовывать элементы столбца. Для этого нам нужно создать словарь, где ключу будет соответствовать старое значение столбца, поэтому ключу будет возвращаться уже значение после преобразования. Давайте попробуем изменить столбец usertypе, так как usertypе содержит только два поля, то мы customer соответственно будем сопоставлять; например, значение 1, и subscriber можем сопоставить значение 2, и у нас появится такой словарь. Далее, давайте применим к нашему столбцу( для этого мы в качестве аргумента MAC передаем словарь) мы видим, что у нас уже возвращается серия из единичек и двоек; таким образом мы уже создали новый сервис в котором изменены значения. 

```python
usertype = {'Customer': 1, 'Subscriber': '2'}
df['usertype'].map(usertype).head()
```

**apply**

**Apply** можно применять ко всему дата сету, например, если мы хотим вытащить минимальное значение для каждого столбца: для этого достаточно в качестве аргумента **apply** передать нужную нам функцию, также с помощью **apply** можно применять преобразования к отдельному столбцу. 

```python
df.apply(min)
```

Столбец содержит время в секундах и с помощью метода apply мы можем его превратить в минуты. 

```python
df['tripduration'].apply(lambda x: x/60).head()
```

Так же apply работает со строками: для этого достаточно указать **exis** равной единице -один из аргументов **apply**. 

```python
df.apply(lambda x: x['tripduration']/60, axis=1).head()
```










