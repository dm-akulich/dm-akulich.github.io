---
layout: post
title: 'Конспект курса Анализ данных в R'
category: R
tags: R
---

# 1.2. Переменные

Переменные - объект в памяти компьютера.

```R
my_var1 <- 42
my_var2 <- 2.45

my_var1 - my_var2
my_var1 + 100

my_var3 = my_var1^2 + my_var2^2
my_var3

>>> my_var3
>>> [1] 1770.003

my_var3 > 200
>>> True

my_var1 == my_var2 # сравнение
>>> False
```

В R присваиваем значение через ```<-```. Но можно и через знак равно.

**Автодополнение по клавише ```TAB```**

*Создание вектора*

```R
my_vector1 <- 1 : 7 # <начало_последовательности>:<конец_последовательности>
my_vector2 <- c(-32, 23, 1, 1.2, 3.14) # с - от слова combine
>>> my_vector2
>>> [1] -32.00  23.00   1.00   1.20   3.14
```

*Обращение к элементу веткора через ИНДЕКС*

```R
my_vector2 <- c(-32, 23, 1, 1.2, 3.14)
my_vector2[1] # по индексу
>>> [1] -32

my_vector2[c(1, 2, 3)] # непосредственно по индексам
>>> [1] -32  23   1

my_vector2[1:3] # слайс
>>> [1] -32  23   1
```

При помощи функции ```с()``` мы можем объединять не только несколько чисел, но сразу несколько векторов (**конкатенация**). Например:

```R
vector_1 <- c(1,2,3)
vector_2 <- c(4,5,6)
vector_3 <- c(vector_1, vector_2)

v1 <- c(1, 3, 5, 7, 9, 0, 0, 0, 0, 20, 3, 5, 8)
v1[v1 > 0]
>>> 1 3 5 7 9

v1[v1 == 0]
>>> 0 0 0 0

v1[v1 < 8]
>>> 1 3 5 7 0 0 0 0

v2 = [v1 < 9 && v1 != 0]
v2
>>> 1  3  5  7  9 20  3  5  8

# Подвыборка испытуемых с ростом выше среднего
v1 <- c(168, 183, 200, 180, 170, 177, 180, 190, 173)
v2 <- v1[v1 > mean(v1)]
>>> 183 200 190
```

**Задача**

В уже созданной переменной *my_vector* хранится вектор из 20 целых чисел.
Найдите сумму всех элементов вектора , которые больше 10. Сохраните сумму в переменную my_sum.

```R
my_sum = sum(my_vector[my_vector >10])
```

**В векторы можно добавлять только данные одного типа.**

### Объединение веткоров

```R
age <- c(16, 18, 22, 27)
is_maried <- c(F, F, T, T)
data = list(age, is_maried) # лист для объединения векторов

data

>>> [[1]]
>>> [1] 16 18 22 27
>>> [[2]]
>>> [1] FALSE FALSE  TRUE  TRUE
```

Индексация по многоуровневому вектору ```data[[1]][2]```  - где ```[[]]``` - индексация по уровню, ```[]``` - индекс элемента.

### Работа с DataFrame

```R
df <- data.frame(Name = name, Age = age, Is_Maried = is_maried) # имя_колонки = массив, имя_колонки = массив и тп
df
>>>   Name Age Is_Maried
>>> 1  olga  16     FALSE
>>> 2 maria  18     FALSE
>>> 3  ania  22      TRUE
>>> 4 kitty  27      TRUE
```

*ЗАДАЧА*: В векторе  my_vector отберите только те наблюдения, которые отклоняются от среднего меньше чем на одно стандартное отклонение. Сохраните эти наблюдения в новую переменную my_vector_2. При этом исходный вектор my_vector оставьте без изменений.

- ```mean(x)``` - среднее значение вектора x
- ```sd(x)``` - стандартное отклонение вектора x
- ```abs(n)``` - абсолютное значение числа n 

```R
my_vector = c(21, 18, 21, 19, 25, 20, 17)
my_vector_2 = my_vector[my_vector < (mean(my_vector)+sd(my_vector)) & my_vector > (mean(my_vector)-sd(my_vector))]
my_vector_2
```

# 1.3 Работа с DataFrame

Считывание 
```R
read.csv("C:/Users/username/documents/evals.csv")
```


**Арифметические операции**

|символ | значение |
---------|--------------
|+     |    сложение |
|-     |   вычитание |
|*     |    умножение |
|/     |    деление  (5 / 2 = 2.5) |
|^ или ** | возведение в степень (5^2 = 25 или 5**2 = 25) |
|x %% y  |  остаток от деления  (5 %% 2 = 1) |
|x %/% y  | целая часть от деления (5 %/% 2 = 2) |

**Логические операции**

|символ | значение |
---------|--------------
| <    |   меньше  |
| <=   |   меньше или равно |
| >     |   больше |
| >=    |   больше или равно |
| ==    |   проверка на равенство |
| !=    |   не равно |
| !x      не x |
| x | y  |  x или y |
| x & y  |  x и y |
| TRUE  | можно сокращенно обозначать T |
| FALSE | можно сокращенно обозначать F |

## Считывание инфы

- Команда ```data(mtcars)``` добавит датасет в рабочую среду. 
- Команда ```help(mtcars)``` выведет информацию о датасете.
- Команда ```my_data <- mtcars``` запишет датасет в новую переменную. 

```R
my_data <- read.csv("/Users/dima/Documents/learn/stepik_analyz_dannyh_v_R_chast_1/datasets/evals.csv")
my_data
```

ТОП-аргументы для функции read: sep="," - разделитель, dec="."- разделитель десятичной дроби, headers="TRUE" - заголовки.

- ```head(my_data, 6)``` - вывести только первые 6 строк;
- ```tail(my_data, 6)``` - вывести только последние 6 строк;
- ```View(my_data)``` - вывести данные как крутую табличку;
- ```str(my_data)``` - посмотреть какие переменные (данные) есть в дата фрейме.
- ```summary(my_data)``` - вывести сводные данные по переменным.
- ```names(my_data)``` - получить только названия переменных. Возвращает вектор значений.

### Обращение к переменным DF

- ```b <- my_data$score``` - обращение к переменной *score*. Возвращает вектор значений переменной;
- ```mean(my_data$score)``` - среднее переменной;
- ```my_data$ten_point_scale <- my_data$score * 2 ``` - присвоить новому столбцу значения score * 2;
- ```my_data$new_variable <- 0``` - сделать столбец с нулями;
- ```my_data$new_variable <- 1:nrow(my_data)``` - столбец с номерами от 1 до конца df.
- ```ncol(my_data)``` или ```nrow(my_data)``` - кол-во строк/столбцов в df
- ```my_data$score[1,10]``` - первые 10 значений вектора
- ```my_data[14:5]``` - значение 14 строки и 5-го столбца
- ```my_data[с(2, 190, 3), 5]``` - значения 2, 190, 3 строки и 5-го столбца
- ```my_data[1:10, 2]``` - первые 10 значений вектора второго столбца
- ```mydata[5,]``` - вся пятая строчка/столбец

### Обращение к переменным DF по условию

- ```my_data$gender == 'female'``` - логический вектор где только женщины
- ```my_data[my_data$gender == 'female', 1]``` - получим только значения той переменной, где ```female=true```
- ```my_data[my_data$gender == 'female', 1:3]``` - получим только значения по первым трем переменным, где ```female=true```
- ```subset(my_data, gender == "female")``` - подвыборка только по женщинам
- ```rbind``` и ```cbind()``` - объединение по строкам и по столбцам.

```R
my_data2 <- subset(my_data, gender == "female")
my_data3 <- subset(my_data, gender == "male")
my_data_2_and_3 <- rbind(my_data2, my_data3)
```



*Памятка* |
---------|--------------
Чтобы изучить структуру данных воспользуйтесь командой str() | str(mtcars)
Чтобы отобрать нужные колонки (переменные) в данных вы можете: | - использовать номера колонок (не забудьте обернуть индексы в вектор): ```mtcars[, c(1, 3, 4)]``` <br> - использовать имена колонок: ```mtcars[, c("mpg", "hp")]```
Чтобы отобрать нужные строки в данных: | ```mtcars[c(1, 5, 7), ]```<br>Эти приемы можно комбинировать: ```mtcars[c(1, 4, 5), c(1, 4)]```
Запомните, сначала идут индексы строк, потом индексы колонок! Также обратите внимание, что мы можем использовать отрицательную индексацию, чтобы отобрать все колонки/строки кроме указанных: | ```mtcars[, -c(3, 4)]``` # отберем все строчки и все колонки кроме 3 и 4. 
Для более сложных запросов используйте функцию subset() | ```subset(mtcars, hp > 100 & am == 1)```
Добавить новую переменную можно при помощи оператора ```$``` |```mtcars$new_var <- 1:32```
Чтобы удалить переменную из данных, используйте такую конструкцию: | ```mtcars$new_var <- NULL```

# 1.3 Элементы синтаксиса

## 1.3.1 Условия

```R
a <- 3
if (a > 0){
  print('positive')
} else {
  print('not positive')
  print(a + 1)
}
>>> "positive"
```

```R
a <-  0
if (a > 0){
  print('positive')
} else if (a < 0){
  print('not positive')
} else {
  print('zero')
}
>>> "zero"
```

*Краткая запись условия*

```R
a <-  1
ifelse(a>0, "positive", "negative")
>>> "positive"

b <- c(1, 2, 4, -2, -1)
ifelse(b>0, "positive", "negative")
>>> "positive" "positive" "positive" "negative" "negative"
```

## 1.3.2 Цикл FOR


```R
for (i in 1:5){
  print(i)
}
>>> 1
>>> 2
>>> 3
>>> 4
>>> 5
```

```R
for (i in 1:nrow(my_data_cars)){
  print(my_data_cars$model[i])
}

>>> "chaser"
>>> "civic"
>>> "cls"
>>> "slk"
>>> "ae86"
>>> "gt86"
>>> "mark"
```

## 1.3.3 FOR + IF

```R
for (i in 1:nrow(my_data_cars)){
  if (my_data_cars$cyl[i] == 4) {
    print(my_data_cars$model[i])
  } 
}
>>> "cls"
>>> "slk"
>>> "ae86"
>>> "mark"
```

```R
for (i in 1:nrow(my_data_cars)){
  if (my_data_cars$cyl[i] == 4) {
    my_data_cars$what[i] <- "has 4 cyl"
  } else {
    my_data_cars$what[i] <- "no 4 cyl"
  }
}
```

```mydata$quality <- rep(NA, nrow(my_data))``` - заполнить нулями переменную

### 1.3.4 WHILE

```R
i <- 5
while (i != 1)  {
  print(i)
  i <- i - 1
}
>>> [1] 5
>>> [1] 4
>>> [1] 3
>>> [1] 2
```

*Задача*: Создайте новую числовую переменную  new_var в данных mtcars, которая содержит единицы в строчках, если в машине не меньше четырёх карбюраторов (переменная "carb") или больше шести цилиндров (переменная "cyl"). В строчках, в которых условие не выполняется, должны стоять нули.

```R
mtcars$new_var <- ifelse(mtcars$carb >= 4 | mtcars$cyl > 6, 1, 0) 

# Или можно так
my_data_cars <- read.csv("/Users/dima/Documents/learn/stepik_analyz_dannyh_v_R_chast_1/datasets/mtcars.csv")
my_data_cars$new_var = rep(NA, nrow(my_data_cars))
for (i in 1:nrow(my_data_cars)){
  if (my_data_cars$cyl[i] >= 4 | my_data_cars$cyl[i] > 6) {
    my_data_cars$new_var[i] <- 1
  } else {
    my_data_cars$new_var[i] <- 0
  }
}
```

# 1.5 Описательные статистики

В DF mtcars есть **номинативная переменная**, которая в R имеет тип numeric ```$ vs  : num  0 0 1 1 0 1 0 1 1 1 ...```. Сделаем из нее факторную.

```R
df = mtcars
df$vs = factor(df$vs, labels=c("V", "S"))
```

Теперь переменная имеет тип Factor ``` $ vs  : Factor w/ 2 levels "V","S": 1 1 2 2 1 2 1 2 2 2 ...```. Значения тоже поменялись.

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/1.png">

**Функции**

- ```median(df$mpg)``` - медиана
- ```sd(df$mpg)``` - 
- ```mean(df$mpg)``` - среднее
- ```range(df$mpg)``` - размах

```R
mean(df$mpg[df$cyl == 6]) # средний расход для авто с 6 цилиндрами
mean(df$mpg[df$cyl == 6 & df$vs == "V"]) # более сложное условие
```

**Функция aggregate()**

```R
aggregate(x = df$hp, by = list(df$vs), FUN = mean)

>>>   Group.1         x
>>> 1       V 189.72222
>>> 2       S  91.35714

mean_hp_vs = aggregate(hp ~ vs, df, mean) # то же самое в виде формулы "ХП по ВС"
```

Мы разбили массив *hp* на группы по *vs* и вычислили среднее в группах. Переименовать заголовки полей можно так ```colnames(mean_hp_vs) = c("VS", "Mean HP")```

*Усложненный вариант с пересечением*

```R
> aggregate(hp ~ vs + am, df, mean)
  vs     am        hp
1  V   Auto 194.16667
2  S   Auto 102.14286
3  V Manual 180.83333
4  S Manual  80.57143
> aggregate(x=df$hp, by=list(df$vs, df$am), FUN=mean) # то же самое
```

*Все колонки кроме*

```R
> aggregate(x=df[,-c(8,9)], by=list(df$vs, df$am), FUN=mean)
  Group.1 Group.2      mpg      cyl     disp        hp     drat       wt     qsec     gear     carb
1       V    Auto 15.05000 8.000000 357.6167 194.16667 3.120833 4.104083 17.14250 3.000000 3.083333
2       S    Auto 20.74286 5.142857 175.1143 102.14286 3.570000 3.194286 19.96714 3.571429 2.142857
3       V  Manual 19.75000 6.333333 206.2167 180.83333 3.935000 2.857500 15.79667 4.666667 4.666667
4       S  Manual 28.37143 4.000000  89.8000  80.57143 4.148571 2.028286 18.70000 4.142857 1.428571

# Формульная запись для группировки
> aggregate(cbind(mpg, disp) ~ am + vs, df, sd)
      am vs      mpg     disp
1   Auto  V 2.774396 71.82349
2 Manual  V 4.008865 95.23362
3   Auto  S 2.471071 49.13072
4 Manual  S 4.757701 18.80213

```

**Задача**: рассчитайте стандартное отклонение переменной hp (лошадиные силы) и переменной disp (вместимости двигателя)  у машин с автоматической и ручной коробкой передач.

```R
aggregate(cbind(hp, disp) ~ am, mtcars, sd)
```

**Импорт пакетов/модулей/библиотек**

```R
> library(psych)
> ?describe
> describe(x = df)
     vars  n   mean     sd median trimmed    mad   min    max  range  skew kurtosis    se
mpg     1 32  20.09   6.03  19.20   19.70   5.41 10.40  33.90  23.50  0.61    -0.37  1.07
cyl     2 32   6.19   1.79   6.00    6.23   2.97  4.00   8.00   4.00 -0.17    -1.76  0.32
disp    3 32 230.72 123.94 196.30  222.52 140.48 71.10 472.00 400.90  0.38    -1.21 21.91
hp      4 32 146.69  68.56 123.00  141.19  77.10 52.00 335.00 283.00  0.73    -0.14 12.12
drat    5 32   3.60   0.53   3.70    3.58   0.70  2.76   4.93   2.17  0.27    -0.71  0.09
wt      6 32   3.22   0.98   3.33    3.15   0.77  1.51   5.42   3.91  0.42    -0.02  0.17
qsec    7 32  17.85   1.79  17.71   17.83   1.42 14.50  22.90   8.40  0.37     0.34  0.32
vs*     8 32   1.44   0.50   1.00    1.42   0.00  1.00   2.00   1.00  0.24    -2.00  0.09
am*     9 32   1.41   0.50   1.00    1.38   0.00  1.00   2.00   1.00  0.36    -1.92  0.09
gear   10 32   3.69   0.74   4.00    3.62   1.48  3.00   5.00   2.00  0.53    -1.07  0.13
carb   11 32   2.81   1.62   2.00    2.65   1.48  1.00   8.00   7.00  1.05     1.26  0.29
```

Рассчитаем описательные статистики с помощью ```describeBy``` - выводит описательные статистики по группам

```R
describeBy(x = df[,-c(8,9)], group = df$vs)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/2.png">

Можно сделать вывод матрицей в DF

```R
descr_2 = describeBy(x = df[,-c(8,9)], group = df$vs, mat = TRUE)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/3.png">

## ЧТО ДЕЛАТЬ С ПРОПУЩЕННЫМИ ЗНАЧЕНИЯМИ

- ```sum(is.na(df$mpg))``` - есть ли пропущенные значения
- ```sum(is.na(df))``` - есть ли пропущенные значения во всем DF
- ```mean(df$mpg, na.rm=T)``` - среднее игнорит пропущенные
- ```aggregate(mpg ~ am, df, sd, na.action=)``` - в аггрегейте можно что-нибудь интересное сделать по аргументу ```na.action=```
- ```describe()``` - по аргументу ```na.rm=T``` - удалит строки с NA, по умолчанию игнорит.


**Заполнить средними пустые значения**

```R
my_vector = c(23,10,16,19,23,22,16,21,24,20,22,21,NA,24,NA,NA,NA,23,24,NA,NA,NA,18)
fixed_vector <- replace(my_vector, is.na(my_vector), mean(my_vector, na.rm = TRUE))
```

# 1.5. Построение графиков

## 1.5.1 Базовые функции для построения графиков

```R
df  <- mtcars
df$vs  <- factor(df$vs  , labels = c("V", "S"))
df$am  <- factor(df$am  , labels = c("Auto", "Manual"))

hist(df$mpg, breaks = 20, xlab = "MPG", main ="Histogram of MPG", 
     col = "green", cex.lab = 1.3, cex.axis = 1.3)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/4.png">

```R
plot(density(df$mpg), xlab = "MPG", main ="Density of MPG", 
     col = "green", cex.lab = 1.3, cex.axis = 1.3)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/5.png">

```R
boxplot(mpg ~ am, df, ylab = "MPG", main ="MPG and AM", 
        col = "green", cex.lab = 1.3, cex.axis = 1.3)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/6.png">

```R
boxplot(df$mpg[df$am == "Auto"], df$mpg[df$am == "Manual"], ylab = "MPG", main ="MPG and AM", 
        col = "green", cex.lab = 1.3, cex.axis = 1.3)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/7.png">

```R
plot(df$mpg, df$hp, xlab = "MPG", ylab ="HP" , main ="MPG and HP", pch = 22)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/8.png">

```R
plot(~ mpg + hp, df) 
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/9.png">

## 1.5.2 Построение графиков на ggplot2

Сначала обратиться к библиотеке ```library(ggplot2)```

- ```ggplot(df, aes(x=mpg))+geom_histogram(fill="white", col="black", binwidth=2)``` - аргументы: (датафрейм, aes()- какие переменные участвуют). ```+geom``` как будем рисовать. 

- ```geom_dotblot``` - точечная
- ```geom_density``` - обычная

```R
ggplot(df, aes(x = mpg))+
  geom_histogram(fill = "white", col = "black", binwidth = 2)+
  xlab("Miles/(US) gallon")+
  ylab("Count")+
  ggtitle("MPG histogram")
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/10.png">

```R
ggplot(df, aes(x = mpg, fill = am))+
  geom_dotplot()+
  xlab("Miles/(US) gallon")+
  ylab("Count")+
  scale_fill_discrete(name="Transmission type")+
  ggtitle("MPG dotplot")
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/11.png">

```R
ggplot(df, aes(x = mpg, fill = am))+
  geom_density(alpha = 0.5)+
  xlab("Miles/(US) gallon")+
  ylab("Count")+
  scale_fill_discrete(name="Transmission type")+
  ggtitle("MPG density plot")
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/12.png">


```R
ggplot(df, aes(x = am, y = hp, fill = vs))+
  geom_boxplot()+
  xlab("Transmission type")+
  ylab("Gross horsepower")+
  scale_fill_discrete(name="Engine type")+
  ggtitle("Gross horsepower and engine type")
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/13.png">


# 2.1 Анализ номинативных данных

Номинативные данные - к какому классу принадлежат наблюдения. R позволяет определять связи между номинативными данными *как образование влияет на выбо партии*.

*Данные*

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/14.png">

Сделаем из переменной status факторную переменную с двумя уровнями

```R
df <- read.csv("/Users/dima/Documents/learn/stepik_analyz_dannyh_v_R_chast_1/datasets/grants.csv")

View(head(df))
str(df)

sd$status = as.factor(df$status)
levels(df$status) = c("Not Founded", "Founded")

df$status = factor(df$status, labels = c("Not Founded", "Founded"))
View(head(df))
```

Что у нас в переменной. Построим таблицу по переменной и таблицу сопряженности *научной области и поддерживается или не поддерживается фондом*

```R
t1 = table(df$status)
t1

dim(t1) # какая размерность таблицы

t2 = table(df$status, df$field) # таблица сопряженности
t2

>>>              beh_cog bio chem physics soc
>>>  Not Founded     100 473   60      70  44
>>>  Founded          65 432   66      78  32

# таблица с названиями

t2 = table(status=df$status, field = df$field)
t2

>>>             field
>>>status        beh_cog bio chem physics soc
>>>  Not Founded     100 473   60      70  44
>>>  Founded          65 432   66      78  32

prop.table(t2) # та же таблица в процентах (но там проценты неудобные)

# можем сказать R, чтобы 100 процентов было по строке (1) или по столбцу (2)

prop.table(t2, 1)
prop.table(t2, 2)

```

Построение графиков (барплот)

```R
barplot(t2, legend.text = TRUE, args.legend = list(x='topright'))
barplot(t2, legend.text = TRUE, args.legend = list(x='topright', beside=TRUE))
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/15.png">
<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/16.png">

Еще есть ```mosaicplot(t2)```

## Биноминальный тест для номинативных переменных

Биноминальный тест позволяет отвеить на вопрос насколько эмпирическое распределение некоторой величины, которая может заканчиваться либо 1 либо 2 отличается от теоретически предсказанного биномиального распределения с заданной вероятностью. Например, подкидываем монетку 20 раз и выпадает 15 орлов и пять решек. Тест позволяет ответить на вопрос является ли монетка нормальной с равновероятным выпадением или она является скошенной в сторону орла. 


```R
# Binomial Test
binom.test(x = 5, n = 20, p = 0.5) # Аргументы: x-интересующие наблюдения, n-наблюдения всего, p-априорная вероятность выпадения.

>>> Exact binomial test
>>> data:  5 and 20
>>> number of successes = 5, number of trials = 20, p-value = 0.04139 ##! p-значение говорит о вероятности того, что в нормальной монетке орел или решка выпадут 5 или менее раз.
>>> alternative hypothesis: true probability of success is not equal to 0.5
>>> 95 percent confidence interval:
>>>  0.08657147 0.49104587
>>> sample estimates:
>>> probability of success 
>>>                   0.25 


binom.test(t1) # отправим таблицу нашу
>>> data:  t1
>>> number of successes = 747, number of trials = 1420, p-value = 0.05268 # если у нас критерий отвержения нулевой гипотезы 0.05, то мы не можем отвергнуть нулевую гипотезу. И сказать что заявки не финансируются чаще, чем финансируются. Проверялась гипотеза: заявки чаще отвергаются чем не отвергаются.
>>> alternative hypothesis: true probability of success is not equal to 0.5
>>> 95 percent confidence interval:
>>>  0.4997023 0.5523023
>>> sample estimates:
>>> probability of success # за успех принято отвержение заявки
>>>              0.5260563
```

## Тест хи-квадрат Пирсона
```R
t1
chisq.test(t1) # скажет насколько отличается эмпирическое распределение от теоретического


>>> Not Founded     Founded 
>>>         747         673 
>>> 
>>> Chi-squared test for given probabilities
>>> 
>>> data:  t1
>>> X-squared = 3.8563, df = 1, p-value = 0.04956 # позволяет нам отвергнуть нулевую гипотезе о том, что вероятности равномерно распределены.

# тест хи возвращает список, у которого есть несколько подобъектов
# chi$exp - ожидаемые значения, если было случайно, то было бы....
# chi$obs - наблюдаемые значения
```

## Тест точный критерий Фишера

Точный критерий Фишера используем когда у нас мало наблюдений в каких-либо ячейках

```R
# Fisher's Exact Test
fisher.test(t2)
```

*Задача*: На основе таблицы HairEyeColor создайте ещё одну таблицу, в которой хранится информация о распределении цвета глаз у женщин-шатенок (Hair = 'Brown'). Проведите тест равномерности распределения цвета глаз у шатенок и выведите значение хи-квадрата для этого теста.

```R
t = HairEyeColor['Brown',,'Female']
chisq.test(t)
```

*Задача*: При помощи критерия Хи - квадрат проверьте гипотезу о взаимосвязи цены (price) и каратов (carat) бриллиантов. Для этого сначала нужно перевести эти количественные переменные в формат пригодный для Хи - квадрат. Создайте две новые переменные в данных diamonds:

- factor_price - где будет 1, если значение цены больше либо равно чем среднее, и 0, если значение цены ниже среднего цены по выборке.
- factor_carat - где будет 1, если число карат больше либо равно чем среднее,  и 0, если ниже среднего числа карат по выборке.

Важный момент - на больших данных цикл for() работает довольно медленно, постарайтесь решить эту задачу без его использования!
Используя эти шкалы при помощи Хи - квадрат проверьте исходную гипотезу. Сохраните в переменную main_stat значение критерия  Хи - квадрат.

```R
my_df = diamonds
my_df$factor_price <- NA
my_df$factor_carat <- NA
my_df$factor_price = ifelse(my_df$price > mean(my_df$price), 1, 0)
my_df$factor_carat = ifelse(my_df$carat > mean(my_df$carat), 1, 0)

main_stat = chisq.test(my_df$factor_price, my_df$factor_carat)[1]
main_stat
```

# 2.2 Сравнение групп

*Будем юзать iris*. Для начала исключим группу setosa из наблюдений, чтобы было две.

```R
df = iris
df1 = subset(df, Species != "setosa")
```

*Сформулируем задачу*: хотим сравнить переменную Sepal.Lenght по переменной Species. Сначала посморим как распеределены данные.

```R
library(ggplot2)

ggplot(df1, aes(x = Sepal.Length))+
  geom_histogram(fill="white", col="black", binwidth = 0.4)+
  facet_grid(Species ~ .) 
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/17.png">

*Другой вариант графика*

```R
ggplot(df1, aes(x = Sepal.Length, fill=Species))+
  geom_density(alpha=0.5)
```

Чтобы посомтреть значимы ли различия между группами с помощью **t-теста**, необходимо, чтобы были соблюдены условия:
- распределение должно быть нормальным
- гомогенность дисперсий

Убедимся, что наблюдаемые отклонения не столь критичны. И можно будет заключить, что распределение нормальное и 

*Проверим распределение переменных*

```R
# Распределение в целом
shapiro.test(df1$Sepal.Length) 
>>> W = 0.98054, p-value = 0.1464 # H0 - о том, что распределение выборки не отличается от нормального. У нас больше 0.05. В целом, распредлеение не отличается от нормального

# Распределение по группе versicolor
shapiro.test(df1$Sepal.Length[df1$Species=="versicolor"])
>>> W = 0.97784, p-value = 0.4647 # не отличается от нормального. р>0.05

# Распределение по группе 
shapiro.test(df1$Sepal.Length[df1$Species=="virginica"])
>>> W = 0.97118, p-value = 0.2583 # не отличается от нормального. р>0.05
```

*Проверим гомогенности дисперсий*

```R
# тест Бартлетта о гомогенности дисперсий
bartlett.test(Sepal.Length ~ Species, df1)
>>> data:  Sepal.Length by Species
>>> Bartlett's K-squared = 2.0949, df = 1, p-value = 0.1478 # p>0.05 - гомогенность дисперсий в двух группах соблюдается
```

Гомогенности дисперсий можно проверять еще тестом Ливина.

Выполним т-тест

```R
t.test(Sepal.Length ~ Species, df1)

>>> Welch Two Sample t-test
>>> data:  Sepal.Length by Species
>>> t = -5.6292, df = 94.025, p-value = 1.866e-07 # p < 0.05 - H0 о равенстве средних отклоняем.
>>> alternative hypothesis: true difference in means is not equal to 0
>>> 95 percent confidence interval:
>>>  -0.8819731 -0.4220269 # 95% доверительный интервал для разницы между средними. Он не включает ноль, Н0 еще раз отклоняем
>>> sample estimates:
>>> mean in group versicolor  mean in group virginica 
>>>                    5.936                    6.588
```

Мы можем использовать т-тест для зависимых выборок и независимых. Например, для зависимой выборки проверим гипотезу о том, что длина (petal.length) и ширина (petal.width) лепестка не равны. Т.е. $$ H_{0} $$ - о том, что показатели равные. В t-тесте это аргумнент *paired=T*

```R
t.test(df1$Petal.Length, df1$Petal.Width, paired = T)

>>> t = 60.552, df = 99, p-value < 2.2e-16 # 
>>> alternative hypothesis: true difference in means is not equal to 0
>>> 95 percent confidence interval:
>>>  3.124156 3.335844
>>> sample estimates:
>>> mean of the differences 
>>>                    3.23 

```

**ПАМЯТКА**

**t-Критерий Стьюдента для независимых выборок**

```R
t.test(Var1 ~ Var2, data) # если первая переменная количественная, а вторая фактор
t.test(data$Var1, data$Var2) # если обе переменные количественные
```

**t-Критерий Стьюдента для зависимых выборок**

```R
t.test(data$Var1, data$Var2, paired = T)
```

**Проверка на нормальность распределения**

```R
shapiro.test(Var1) # проверка на нормальность распределения переменной Var1
# но не удобно когда есть группирующая факторная переменная
```

**Поможет функция by(), которая применяет различные функции на каждом уровне фактора.**

```R
by(iris$Sepal.Length, INDICES = iris$Species, shapiro.test) # проверка на нормальность переменной 
# Sepal.Length в трех разных группах в соответствии с переменной Species
```

**Проверка на гомогенность дисперсий**

```R
bartlett.test(mpg ~ am, mtcars) #Критерий Бартлетта 
```

## Как нам визуализировать результаты

Построим на графике два средних, два доверительных интервала и чтобы это было красиво

```R
ggplot(df1, aes(x=Species, Sepal.Length))+
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width=0.1)+
  stat_summary(fun.y = mean, geom = "point", size=4)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/19.png">

## Непараметрический аналог критерия т-Стьюдента. Тест Вилкоксона-Манна-Уитни (иногда называют Манна Уитни)

**Потребность использовать тест может возникнуть, когда не выполняется одно из требований к тесту, немного данных или не очень нормальное распределение.**

```R
wilcox.test(Petal.Length ~ Species, df1)
>>> W = 44.5, p-value < 2.2e-16
```

**Сначала с помощью теста Бартлетта проверяем гомогенность дисперсий двух выборок. В случае, если дисперсии значимо не отличаются (с уровнем 0.05), применяем тест Стьюдента, иначе - непараметрический тест (Манна-Уитни)**

**Задача 1**: *В этом задании нужно проверить гипотезу о равенстве средних двух выборок, загрузив набор данных (нажмите начать решать задание) и выполнив все необходимые операции на вашем компьютере. В скачанных данных вы найдете две переменные: количественную переменную, и номинативную переменную с двумя градациями (которая разделяет наблюдения на две группы).Сначала с помощью теста Бартлетта проверьте гомогенность дисперсий двух выборок. В случае, если дисперсии значимо не отличаются (с уровнем 0.05), примените тест Стьюдента, иначе - непараметрический тест (Манна-Уитни). В поле для ответа введите получившийся p-value, с точностью четыре знака после запятой. Обратите внимание, что по умолчанию в t.test стоит var.equal = FALSE, так как мы будем применять его только в случае гомогенности дисперсий, измените значение этого параметра на  var.equal = TRUE.*

```R
df$V2 = factor(df$V2)
# сравним распределение
ggplot(df, aes(x=V1, fill=V1))+
  geom_histogram(fill="white", col="black", binwidth = 0.4)
# Проверка нормальности распределения
shapiro.test(df$V1)
# Проверка гомогенности дисперсии двух выборок Бартлетт
bt = bartlett.test(V1 ~ V2, df)$p.value
bt
if (bt > 0.05) {
  t.test(V1 ~ V2, df, var.equal=T)
} else {
  wilcox.test(V1 ~ V2, df)
}
```

# 2.3 Дисперсионный анализ

Дисперсионный анализ - инструмент для сравнения нескольких групп. 

*Справка по дисперсионному анализу*.

Для чего применяется дисперсионный анализ? Цель дисперсионного анализа - исследование наличия или отсутствия существенного влияния какого-либо качественного или количественного фактора на изменения исследуемого результативного признака. Для этого фактор, предположительно имеющий или не имеющий существенного влияния, разделяют на классы градации (говоря иначе, группы) и выясняют, одинаково ли влияние фактора путём исследования значимости между средними в наборах данных, соответствующих градациям фактора. Примеры: исследуется зависимость прибыли предприятия от типа используемого сырья (тогда классы градации - типы сырья), зависимость себестоимости выпуска единицы продукции от величины подразделения предприятия (тогда классы градации - характеристики величины подразделения: большой, средний, малый).

Минимальное число классов градации (групп) - два. Классы градации могут быть качественными либо количественными.

Как формулируются, принимаются и отвергаются гипотезы при дисперсионном анализе? При дисперсионном анализе определяют удельный вес суммарного воздействия одного или нескольких факторов. Существенность влияния фактора определяется путём проверки гипотез:

$$ H_{0}: \nu_{1} = \nu_{2} = ... = \nu_{a} $$, где a - число классов градации - все классы градации имеют одно значение средних,
$$ H_{1}: $$ не все $$ \nu_{i} $$ равны - не все классы градации имеют одно значение средних.
Если влияние фактора не существенно, то несущественна и разница между классами градации этого фактора и в ходе дисперсионного анализа нулевая гипотеза H0 не отвергается. Если влияние фактора существенно, то нулевая гипотеза H0 отвергается: не все классы градации имеют одно и то же среднее значение, то есть среди возможных разниц между классами градации одна или несколько являются существенными.

Основная логика дисперсионного анализа. Подводя итоги, можно сказать, что целью дисперсионного анализа является проверка статистической значимости разницы между средними (для групп или переменных). Эта проверка проводится с помощью анализа дисперсии, т.е. с помощью разбиения общей дисперсии (вариации) на части, одна из которых обусловлена случайной ошибкой (то есть внутригрупповой изменчивостью), а вторая связана с различием средних значений. Последняя компонента дисперсии затем используется для анализа статистической значимости различия между средними значениями. Если это различие значимо, нулевая гипотеза отвергается и принимается альтернативная гипотеза о существовании различия между средними.

Зависимые и независимые переменные. Переменные, значения которых определяется с помощью измерений в ходе эксперимента (например, балл, набранный при тестировании), называются зависимыми переменными. Переменные, которыми можно управлять при проведении эксперимента (например, методы обучения или другие критерии, позволяющие разделить наблюдения на группы) называются факторами или независимыми переменными.

Дисперсионный анализ – анализ изменчивости признака под влиянием каких-либо контролируемых переменных факторов.

Метод однофакторного дисперсионного анализа применяется в тех случаях, когда исследуются изменения результативного признака (зависимой переменной) под влиянием изменяющихся условий или градаций какого- либо фактора.

Влиянию каждой из градаций фактора подвержены разные выборки. Должно быть не менее трех градаций фактора и не менее двух наблюдений в каждой градации.

Ковариата (дополнительная информация) выступает в виде независимой метрической переменной и используется для удаления посторонней вариации из зависимой переменной, поскольку самыми важными являются эффекты факторов.

Вариацию в зависимой переменной, обусловленную ковариатой, удаля- ют корректировкой среднего значения зависимой переменной в пределах каждого из факторов (условий эксперимента). Затем, исходя из скорректированных оценок, выполняют дисперсионный анализ.

*Примеры формулировки задач*: 

- При уровне значимости α=0,05 методом дисперсионного анализа проверить нулевую гипотезу о влиянии фактора на качество объекта на основании пяти измерений для трех уровней фактора Ф1 – Ф3.
- В течение шести лет использовались четыре различных технологии по выращиванию сельскохозяйственной культуры. Необходимо установить влияние различных технологий на урожайность культуры по данным таблиц. Задачу просчитать вручную и на ПЭВМ.


```R
DV ~ IV # One-way. Одна переменная (IV) позволяет предсказывать другую (DV)

DV ~ IV1 + IV2 # Two-way. На одну переменну (DV) влияют две позволяет предсказывать другую (DV)

DV ~ IV1:IV2 # Two-way interaction. Когда зависимая переменая (DV) зависит от IV, на которую влияет другая переменная (IV2)

DV ~ IV1 + IV2 + IV1:IV2 # Main effects + interactions. Формула с главными эффектами + взаимодействие

DV ~ IV1 * IV2 # то же, что и выше, просто короче запись

DV ~ IV1 + IV2 + IV3  + IV1:IV2

DV ~ (IV1 + IV2 + IV3)^2 # такая запись говорит, что у нас три предиктора и нас интересует взимодействие до второго уровня (между IV1:IV2, IV1:IV3, IV2:IV3).

DV ~ IV1 + Error(subject/IV1) # диспрерсионный анализ для повторных изерений. Если у нас есть межгрупповая дисперсия. Если мы измерили некоторые характеристики на одном испытуемом несколько раз. И то же самое сделали с другим испытуемым.
```

## Примеры дисперсионного анализа в R

### One-Way ANOWA

Данные. Сравнение цен на разные продукты в разных магазинах.

```R
str(mydata)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/20.png">

Будем смотреть какие переменные влияют на цены в магазине. 

```R
# One Way ANOVA
boxplot(price ~ origin, data = mydata)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/21.png">

Медиана импортной продукции выше, чем медиана местной продукции.

```R
fit = aov(price ~ origin, data = mydata)
summary(fit)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/22.png">

Видим влияние нашего фактора origin и остатки (Residuals), F-знач и Р-знач. Видим, что Fзнач=6,65, а p=0.0189, что позволяет **отвергнуть $$ H_{0} $$ о том, что цены на импортные продукты и народные равны**.

### Two-Way ANOWA

```R
# TWO-way ANOWA
fit <- aov(price ~ origin + store, data=mydata)
summary(fit)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/23.png">

В данном случае опять видим, что страна происхождения значима, а тип магазина никак не влияет на конечную цену продукта.

Чтобы посомтреть дополнительную инфу по модели, можум использовать функцию ```model.table(fit, "means")```, указав, какая метрика нас интересует. В данном случае means.


### Анализ взаимодействия

Визуализируем данные модели two-way ANOWA ```fit <- aov(price ~ origin + store, data=mydata)```.

```R
pd=position_dodge(0.1)
ggplot(mydata, aes(x=store, y=price, color=origin, group=origin))+
  stat_summary(fun.data=mean_cl_boot, geom="errorbar", width=0.2, lwd=0.8, position = pd) +
  stat_summary(fun.data=mean_cl_boot, geom="line", size=1.5, position = pd) +
  stat_summary(fun.data=mean_cl_boot, geom="point", size=5, pch=15) +
  theme_bw()
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/24.png">

Опишем содель взаимодействия

```R
fit <- aov(price ~ origin * store, data=mydata)
summary(fit)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/25.png">

Снова видим, что страна производства значима, тип магазина опять не влияет, а вот взаимодейтсвие оказывается значимым (F>5, p<0.05>). Видим что взаимодействие значимо.

**Задача 1**:*Воспользуемся встроенными данными npk, иллюстрирующими влияние применения различных удобрений на урожайность гороха (yield). Нашей задачей будет выяснить, существенно ли одновременное применение азота (фактор N) и фосфата (фактор P). Примените дисперсионный анализ, где будет проверяться влияние фактора применения азота (N), влияние фактора применения фосфата (P) и их взаимодействие. В ответе укажите p-value для взаимодействия факторов N и P.*

```R
mydata = npk
fit = aov(yield~N*P, mydata)
summary(fit)
```

В отличие от Т-теста, дисперсионный анализ хорош тем, что туда можно загонять сразу много групп (не только два типа магазина или два чего-нибудь еще). Но когда мы получаем значимые различия между пятью группами, мы не можем точно сказать между какими конкретно уровнями переменной существую различия. Для этого использую попарные сравнения. Сейчас рассмотрим, каким образом можно сделать попарные сравнения с поправкой на множественное сравнение.

*Как влияют разные типы еды на нашу переменную*

```R
mydata <- read.csv("/datasets/shops.csv")

ggplot(mydata, aes(x=food, y=price)) +
  geom_boxplot()
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/26.png">

самая дорогая еда - сыр, самая низкая цена - хлеб, 

```R
fit = aov(price~food, data = mydata)
summary(fit)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/27.png">

Видим, что переменная еда является значимым предиктором. То есть наши продукты различаются по цене. Но пока не можем сказать, какие именно группы продуктов различаются между собой. Для этого используем множественное сравнение.

```R
TukeyHSD(fit)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/28.png">

В первом столбце смотрим группы, которые сравниваются. Во втором столбце видим различия в абсолютных числах. Далее в 3-м и 4-м доверительные интервалы. И в последнем p-значение. Для всех группы сыр-хлеб видим, что p-значение маленькое и отвергаем нулевую гипотезу. Различие между ценой на сыр и на хлеб статистически значимо. Остальыне различия статистически незначимы.

### Дисперсионный анализ с повторными измерениями

Он необходим, когда наши наблюдения не независимы, а сгруппированы по каким-то переменным. Например, есть испытуемые, каждый и которых проходил несколько тестов. И чтобы учесть что испытуемые в целом между собой различаются. Нужно применять АНОВУ ис повторными измерениями. 

Датасет про психологию.
В данном случае каждый испытуемый проходил три вида терапии. Исследуем влияение вида терапии на результат

```R
mydata2$subject = as.factor(mydata2$subject)

fit1 = aov(well_being ~ therapy + Error(subject/therapy), data = mydata2)
summary(fit1)
```

**Задача 1**: *В этой задаче вам дан набор данных, в котором представлена информация о температуре нескольких пациентов, которые лечатся разными таблетками и у разных врачей. Проведите однофакторный дисперсионный анализ с повторными измерениями: влияние типа таблетки (pill) на температуру (temperature) с учётом испытуемого (patient). Каково p-value для влияния типа таблеток на температуру?*

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/29.png">

```R
df_pil$pill = as.factor(df_pil$pill)
df_pil$doctor = as.factor(df_pil$doctor)
df_pil$patient = as.factor(df_pil$patient)
fit = aov(df_pil$temperature ~ df_pil$pill + Error(patient/pill), data=df_pil)
summary(fit)
```

**Задача 2**: *Теперь вашей задачей будет провести двухфакторный дисперсионный анализ с повторными измерениями: влияние факторов doctor, влияние фактора pill и их взаимодействие на temperature. Учтите обе внутригрупповые переменные: и тот факт, что один и тот же больной принимает разные таблетки, и тот факт, что  один и тот же больной лечится у разных врачей! Каково F-значение для взаимодействия факторов доктора (doctor) и типа таблеток (pill)?*

```R
df_pil$pill = as.factor(df_pil$pill)
df_pil$doctor = as.factor(df_pil$doctor)
df_pil$patient = as.factor(df_pil$patient)
fit = aov(df_pil$temperature ~ df_pil$pill*df_pil$doctor + Error(patient/pill + patient/doctor), data=df_pil)
summary(fit)
```


# 2.4 Создание собственных функций в R

```R
my_calc <- function(x, y) {
  s <- x+ y
  d <- x - y
  return(c(s,d))
}

my_calc(x=4, y=2)
```


```R
my_na_rm <- function(x) {
  x[is.na(x)] <- mean(x, na.rm = T)
  return(x)
}
```

```R
my_na_rm <- function(x) {
  if (is.numeric(x)) {
    x[is.na(x)] <- mean(n, na.rm = T)
    return(x)
  } else {
    print("X is not numeric")
  }
}

my_na_rm(x=c("2", "3", NA))
>>> "X is not numeric"
```

```R
my_na_rm <- function(x) {
  if (is.numeric(x)) {
    stat_test <- shapiro.test(x)
    if(stat_test$p.value > 0.05) { # если распределение прошло проверку на нормальность
      x[is.na(x)] <- mean(x, na.rm = T)
      print("NA values were replaced with mean")
    } else {
      x[is.na(x)] <- median(x, na.rm = T)
      print("NA values were replaced with median")
    }
    return(x)
  } else {
    print("X is not numeric")
  }
}

d1 <- rnorm(2000) # нормальное распределение
d2 <- runif(2000) # равномерное распределение

d1[1:10] <- NA
d2[1:10] <- NA

d1 <- my_na_rm(d1)
>>> [1] "NA values were replaced with mean"
d2 <- my_na_rm(d2)
>>> [1] "NA values were replaced with median"
```

**Испорт функции из модуля**

```R
source("my_na_rm.R")
```

**Задача 1**: *Напишите функцию, которая выводит номера позиций пропущенных наблюдений в векторе. На вход функция получает числовой вектор с пропущенными значениями. Функция возвращает новый вектор с номерами позиций пропущенных значений.*

```R
NA.position <- function(x){
  na_positions = which(x %in% c(NA))
  return(na_positions)
}
```


**Задача 2**: *Напишите функцию NA.counter для подсчета пропущенных значений в векторе.*

```R
NA.counter <- function(x){
  c = is.na(c) 
  return(length(c[c==T]))
}

```

Как из нескольких данных собрать один датафрейм

```R
dir(pattern = "*.csv") # выводит все файлы в диреткории

grants <- data.frame()

for (i in dir(pattern = "*.csv")){
  temp_df <- read.csv(i)
  grants <- rbind(temp_df, grants)
}
```

**Задача 3**: *Напишите функцию outliers.rm, которая находит и удаляет выбросы. Для обнаружения выбросов воспользуемся самым простым способом, с которым вы не раз встречались, используя график Box plot. Выбросами будем считать те наблюдения, которые отклоняются от 1 или 3 квартиля больше чем на 1,5 *  IQR, где  IQR  - межквартильный размах. На вход функция получает числовой вектор x. Функция должна возвращать модифицированный вектор x с удаленными выбросами.*

```R
outliers.rm <- function(x){
  quantiles = quantile(x, probs = c(0.25, 0.75))
  return(x[x > quantiles[1] - 1.5*IQR(x) & x < 1.5*IQR(x)+quantiles[2]])
}
```

# 3.1 Корреляция и простая линейная регрессия (МНК)

```R
df <- mtcars
cor.test(x=df$mpg, y=df$hp) # корреляция между ХП и миль/топливом

>>> t = -6.7424, df = 30, p-value = 1.788e-07
>>> alternative hypothesis: true correlation is not equal to 0
>>> 95 percent confidence interval:
>>>  -0.8852686 -0.5860994
>>> sample estimates:
>>>        cor 
>>> -0.7761684 
```

Если нам нужен непараметрический критерий, а не Пирсона, то можно использовать аргумент ```method="kendal"```, ```spearman```.

```R
cor.test(~ mpg + hp, df) # то же, тоолько другой записью
```

**Попарный анализ количественных переменных**

```R
df <- mtcars
df_numeric <- df[,c(1,3:7)]
pairs(df_numeric)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/30.png">

Можно рассчитать коэфициент корееляции сразу между всеми переменными

```R
cor(df_numeric)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/31.png">

```R
library(psych)
corr.test(df_numeric) # показывает на кореляцию и р-уровень
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/32.png">

**Задача**: *Напишите функцию corr.calc, которая на вход получает data.frame с двумя количественными переменными, рассчитывает коэффициент корреляции Пирсона и возвращает вектор из двух значений: коэффициент корреляции и p - уровень значимости.*

```R
library(psych)
corr.calc <- function(df) {
  x = corr.test(df[c(1)], df[c(2)])
  return(c(x$r, x$p))
}
```

**Задача**: *Напишите функцию smart_cor, которая получает на вход dataframe с двумя количественными переменными. Проверьте с помощью теста Шапиро-Уилка, что данные в обеих переменных принадлежат нормальному распределению. Если хотя бы в одном векторе распределение переменной отличается от нормального (p - value меньше 0.05), то функция должна возвращать коэффициент корреляции Спирмена. (Числовой вектор из одного элемента). Если в обоих векторах распределение переменных от нормального значимо не отличается, то функция должна возвращать коэффициент корреляции Пирсона.*

```R
smart_cor <- function(test_data){
  if (shapiro.test(test_data$col1)$p < 0.05 | shapiro.test(test_data$col2)$p < 0.05) {
    k = cor.test(test_data$col1, test_data$col2, method = "spearman")
    return(k$estimate)
  }
  else {
    k = cor.test(test_data$col1, test_data$col2)
    return(k$estimate)
  }
}
```

**Пример1** Построим самую простую модель с одним предиктором.

```R
df <- mtcars
df_numeric <- df[,c(1,3:7)]

fit <- lm(mpg ~ hp, df)
summary(fit)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/33.png">

Построим для этой модели линию тренда.

```R
library(ggplot2)
ggplot(df, aes(hp,mpg, col=factor(am)))+
  geom_point(size=5)+
  geom_smooth(method = "lm")
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/34.png">

В нашей модели рассмотрим предсказанные значения. Сделаем df и сравним предсказанные значения с реальными.

```R
fitted_values_mpg <- data.frame(mpg = df$mpg, fitted = fit$fitted.values)
```

Теперь предскажем значения для новых значений на основе модели

```R
new_hp <- data.frame(hp=c(100,150, 129,300))
new_hp$mpg <- predict(fit, new_hp)
```

**Пример 2** Теперь построим другую модель регрессии, где в качестве **независимой переменной** будет номинативная переменная. Это уже по сути будет дисперсионный анализ.

```R
my_df <- mtcars
my_df$cyl <- factor(my_df$cyl, labels = c("four", "six", "eight"))

fit <- lm(mpg ~ cyl, my_df)
summary(fit)

ggplot(my_df, aes(cyl, mpg))+
  geom_point()+
  geom_smooth(method = "lm")+
  theme(axis.text = element_text(size = 25),
        axis.title = element_text(size = 25, face = "bold"))
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/35.png">
<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/36.png">

Теперь мы фактически сравниваем группы между собой. *Estimate* в данном случае - это среднее значение внутри группы *fourcyl*. *cylsix* это отклонение от *cylfour* - это типа отклонение от базового уровня.

**Задача**: *Скачайте набор данных - dataframe с двумя **количественными** переменными (вспомните при необходимости, как задавать разделитель и другие параметры функции read.table), постройте линейную регрессию, где - первая переменная - зависимая, вторая - независимая. В ответ укажите значения регрессионных коэффициентов сначала intercept затем  slope.*

```R
fit <- lm(V1 ~ V2, my_df)
summary(fit)

ggplot(my_df, aes(V1, V2))+
  geom_point()+
  geom_smooth(method = "lm")+
  theme(axis.text = element_text(size = 5),
        axis.title = element_text(size = 25, face = "bold"))
```

**Задача**: *Воспользуемся уже знакомыми данными diamonds из библиотеки ggplot2. Только для бриллиантов класса Ideal (переменная cut) c числом карат равным 0.46 (переменная carat) постройте линейную регрессию, где в качестве зависимой переменной выступает price, в качестве предиктора - переменная  depth. Сохраните коэффициенты регрессии в переменную fit_coef.*

```R
df_diamonds = diamonds
df_diamonds_ideal = subset.data.frame(df_diamonds, cut == "Ideal" & carat == 0.46)
fit <- lm(price ~ depth, df_diamonds_ideal)
# summary(fit)
fit_coef = fit$coefficients
```

**Задача**:*Напишите функцию regr.calc, которая на вход получает dataframe c двумя переменными. Если две переменные значимо коррелируют (p - уровень значимости для коэффициента корреляции Пирсона меньше 0.05), то функция строит регрессионную модель, где первая переменная - зависимая, вторая - независимая. Затем создает в dataframe новую переменную с назанием fit, где сохраняет предсказанные моделью значения зависимой переменной. В результате функция должна возвращать исходный dataframe с добавленной новой переменной fit. Если две переменные значимо не коррелируют, то функция возвращает строчку "There is no sense in prediction"*

```R
regr.calc <- function(sample_data){    
	cor_result = cor.test(~sample_data[[1]] + sample_data[[2]])    
	if (cor_result$p.value < 0.05){    
		fit_model  <- lm(sample_data[[1]] ~ sample_data[[2]])    
		sample_data$fit  <- fit_model$fitted.values    
	return(sample_data)    
	} else {    
		return('There is no sense in prediction')}}
```



# 3.2 Множественная линейная регрессия 

### Множественная линейная регрессия с числовыми предикторами

Испльзуемый встроенный датасет swiss. Предстказывать будем переменную Fertility(num). Переменная распределена нормально. Будем строить зависимость от переменных Examination (int), и Chatholic(num)

```R
# numeric predictors
swiss <- as.data.frame(swiss)

str(swiss)
hist(swiss$Fertility, col="red")


fit <- lm(Fertility ~ Examination + Catholic, data = swiss)
summary(fit)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/37.png">

Значимо предсказывает рождаемость только физическая подготовка. Если нужно будет посмотреть взаимодействие факторов, то смотрим с помощью ```Fertility ~ Examination * Catholic```.

Построим доверительные интервалы для модели.

```R
confint(fit)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/38.png">

**Задача**:*Напишите функцию fill_na, которая принимает на вход данные с тремя переменными: x_1  -  числовой вектор, x_2 - числовой вектор, y - числовой вектор с пропущенными значениями. Теперь — самое интересное. На первом этапе, используя только наблюдения, в которых нет пропущенных значений, мы построим регрессионную модель (без взаимодействий), где  y — зависимая переменная, x_1 и x_2 — независимые переменные. Затем, используя построенную модель, мы заполним пропущенные значения предсказаниями модели. Функция должна возвращать dataframe c новой переменной  y_full. Сохраните в нее переменную y, в которой пропущенные значения заполнены предсказанными значениями построенной модели.*

```R
fill_na <- function(my_df){    
	fit <- lm(y ~ x_1+x_2, my_df)    
	my_df$y_full = ifelse(is.na(my_df$y), predict(fit, my_df), my_df$y)    
	return(my_df)}
```

**Задача**: *В переменной df сохранен subset данных mtcars только с переменными "wt", "mpg", "disp", "drat", "hp". Воспользуйтесь множественным регрессионным анализом, чтобы предсказать вес машины (переменная "wt"). Выберите такую комбинацию независимых переменных (из "mpg", "disp", "drat", "hp"), чтобы значение R^2 adjusted было наибольшим. Взаимодействия факторов учитывать не надо.*

```R
model <- lm(wt ~ mpg + disp + hp, df)
```

**Задача**: *Воспользуйтесь встроенным датасетом attitude, чтобы предсказать рейтинг (rating) по переменным complaints и critical. Каково t-значение для взаимодействия двух факторов?*

```R
df = attitude
model = lm(rating ~ complaints*critical, df)
summary(model)
```

## Линейная регрессия с категориальными предикторами

В датасете swiss их нет. Но мы сделаем из переменной catholic.

```R
hist(swiss$Catholic, col="red")
swiss$religious <- ifelse(swiss$Catholic > 60, 'Lots', 'Few')
swiss$religious <- as.factor(swiss$religious)
fit3 <- lm(Fertility ~ Examination + religious, data = swiss)
summary(fit3)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/39.png">

В (intercept) у нас хранится среднее предсказанное значение зависимой переменной для первого уровня категориальной переменной (few первая по алфавиту) при том что непрерывные переменные равны нулю. В (intercept) у нас среднее предсказанное значение рождаемости для провинции, где у нас мало католиков. Examination отноится только к religious - few. 

Хард начинается когда речь идет о взаимодействии непрерывных переменных и категориальынх. Сделаем модель с таким взаимодействием.

```R
swiss$religious <- ifelse(swiss$Catholic > 60, 'Lots', 'Few')
swiss$religious <- as.factor(swiss$religious)
fit3 <- lm(Fertility ~ Examination * religious, data = swiss)
summary(fit3)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/40.png">

В (Intercept) хранится значение для religious-Few. Т.е. среднее предсказанное значение для тех провинций, в которых мало католиков, и при этоу examination на нуле. Коэфициент физ. подготовки (examination) говорит, насколько изменяется в среднем рождаемость при увеличении физ подготовки на единицу только в тех провинциях где мало католиков. 

ReligiousLots говорит о том, насколько изменяется предсказанное значение рождаемости при переходе от провинции где у нас мало католиков, к провинциям где много католиков. Что значит взаимодействие факторов (examination:religiousLots), оно говоррит о том, насколько физическая подготовка влияет на рождаемость в тех провинциях, где много католиков (1.0096).  

Рассмотрим графики зависимости на примере влияния переменных Examination, religious и их взаимодействия на рождаемость (Fertility).

```R
ggplot(swiss, aes(x = Examination, y = Fertility)) + 
  geom_point() + 
  geom_smooth(method = 'lm')
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/41.png">

Видим зависимость с узким доверительным интервалом. 

Рассмотрим модель с несколькими непрерывными переменными и одним категориальным предиктором.

У нас будет категориальный предиктор religious, детская смертность и физ.подготовка.

```R
fit5 <- lm(Fertility ~ religious*Infant.Mortality*Examination, data = swiss)
summary(fit5)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/42.png">

По результатам видим, что:

- В (intercept) хранятся средние предсказанные значения рождаемости для областей где мало католиков (religiousFew) - 42.58729.
- ReligiousLots насколько изменяется рождаемость, когда переходим от religiousFew к religiousLots (63.11979). Но результат все равно не значим.
- Infant.Mortity - насколько влияет детская смертность на рождаемость.
- Examination - и насколько физ подготовка влияет.
- Infant.Mortity и Examination отностится только к religiousFew
- religiousLots:Examination и religiousLots:Examination это детская смертность и физ подготовка, которые влияют на рождаемость в районах religiousLots.
- Infant.Mortality:Examination - относятся только к тому, что в (intercept)
- religiousLots:Infant.Mortality:Examination - относится к religiousLots

**Памятка по интерпретации результатов регрессионного анализа с категориальными и непрерывными переменными**

Модель для примера: ```DV ~ IV_numeric * IV_categorical```

```IV_categorical``` - фактор с двумя уровнями (Level1 и Level2)

Коэффициенты:

- *Intercept* — предсказанное значение DV для первого уровня IV_categorical с учётом того, что IV_numeric равна нулю.
- *IV_numeric* — насколько изменяется предсказанное значение DV при увеличении IV_numeric на одну единицу в группе, соответствующей первому уровню IV_categorical
- *IV_categoricalLevel2* — насколько изменяется предсказанное значение DV при переходе от первого уровня IV_categorical ко второму уровню. С учётом того, что IV_numeric равна нулю. 
- *IV_numeric:IV_categoricalLevel2* — насколько сильнее (или слабее) изменяется предсказанное значение DV при увеличении IV_numeric на одну единицу в группе, соответствующей второму уровню IV_categorical, по сравнению с первым уровнем. 

*Как предсказывать значения в новом датасете на основе полученных коэффициентов*

**1).** Предположим у нас есть новый объект, про который мы знаем, что он принадлежит к группе, соответствующей IV_categorical (Level1) и измеренный у него IV_numeric составил 10:

```Предсказанное значение DV = Intercept + 10 * IV_numeric```

**2).** Предположим у нас есть новый объект, про который мы знаем, что он принадлежит к группе, соответствующей IV_categorical (Level2) и измеренный у него IV_numeric составил 6:
 
```Предсказанное значение DV = Intercept + IV_categoricalLevel2 + 6 * (IV_numeric + IV_numeric:IV_categoricalLevel2)```

**Задача**: *В этом примере будем работать с хорошо вам известным встроенным датасетом mtcars. Переменная am говорит о том, какая коробка передач используется в машине: 0 - автоматическая, 1 - ручная. Сделаем эту переменную факторной. mtcars$am <- factor(mtcars$am, labels = c('Automatic', 'Manual')). Теперь постройте линейную модель, в которой в качестве зависимой переменной выступает расход топлива (mpg), а в качестве независимых - вес машины (wt) и коробка передач (модифицированная am), а также их взаимодействие. Выведите summary этой модели.*

```R
k = mtcars
k$am <- factor(k$am, labels = c('Automatic', 'Manual'))
head(mtcars)

fit <- lm(mpg ~ wt*am,k)
summary(fit)

ggplot(k, aes(x=wt, y=mpg, col=am))+
  geom_point() +
  geom_smooth(method = "lm")
```


# 3.3 Множественная линейная регрессия. Отбор моделей

Сделаем модель со всеми предикторами.

```R
swiss <- data.frame(swiss)
fit_full <- lm(Fertility ~ ., data = swiss)
summary(fit_full)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/43.png">

Все предикторы оказались значимыми, кроме Examination. 

Теперь построим модель со всеми предикторами, кроме Agriculture.

```R
fit_reduced <- lm(Fertility ~ Infant.Mortality + Examination + Education + Catholic, data = swiss)
summary(fit_reduced)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/44.png">

Физ подготовка опять не значима, все остальное значимо в качестве предиктора.

- В первой модели $$ R^{2} = 0.7 $$, Исправленный $$ R^{2} = 0.671 $$.
- Во второй модели $$ R^{2} = 0.6639 $$, Исправленный $$ R^{2} = 0.631 $$.

Чтобы понять, насколько значимо различаются доли дисперсий, которые объясняются обеими моделями, можем применить дисперсионный анализ. И сравнить ддоли дисперсий, которые объясняют обе модели. Применим функцию ANOVA в R.

```R
anova(fit_full, fit_reduced) # аргументы - две модели
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/45.png">

Для второй модели F значение достаточно большое, а Pr(>F)-значение значимо. Доля дисперсии, объясняемая полной моделью значимо больше, чем доля дисперсии, объясняемая урезанной моделью. Среди двух моделей выбираем полную. 

В примере у нас есть 2 модели: полная и неполная. У первой предикторы объясняют 70% вариации (дисперсии) зависимой переменной, т.к. R2=0,70 (исправденный 0,67), у второй R2 = 0.66 (исправленный 0.63 ). Разница между ними - достаточно мала. Дисперсионный анализ дает ответ на вопрос "Значимо ли отличаются дисперсии у двух моделей?". F-критерий говорит, что значимо, т.е. разница есть. Соответственно выбирается та модель, которая лучше объясняет зависимую переменную, т.е. та, у которой R2 больше. если p > 0.05, то это статистически не значимые различия. R2 Adjusted рассчитывается с поправкой на множественные сравнения, т.е. если Вы проверите  влияние на зависимую переменную от множества величин, то вероятность случайно получить какое-то серьезное влияние там, где его нет, растет с количеством переменных, которые Вы проверяете.

*Еще пример*. Сделаем еще одну модель со всеми предикторами, кроме Examination.

```R
fit_reduced2 <- lm(Fertility ~ Infant.Mortality + Education + Catholic + Agriculture, data = swiss)
summary(fit_reduced2)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/46.png">

Все предикторы значимы. Срасним вторую урезанную модель и полную

```R
anova(fit_full, fit_reduced2)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/47.png">

В данном случае p-значение **не достигает уровня отвержения нулевой гипотезы**. В данном случае, это гипотеза о том, что две модели примерно одинаково объясняют долю дисперсий.

**В R есть функция ```step()```, которая помогает отбирать предикторы.**

```R
optimal_fit <- step(fit_full, direction = "backward")
summary(optimal_fit)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/48.png">

функция step позволяет нам подобрать модель с оптимальным количеством предикторов. С помощью аргумента scope мы можем задать пространство моделей с разным числом предикторов, в котором будет происходить поиск оптимального набора предикторов. Самый простой путь - задать границы возможных моделей с помощью нулевой и полной моделей.

scope = list(lower = model_null, upper = model_full)
Аргумент direction позволяет задать направление поиска. 

Первый аргумент (object) задаёт начальную модель, с которой начинается поиск. Обратите внимание на то, что при разных значениях аргумента direction нужно использовать разные начальные модели. 

Функция step возвращает оптимальную модель.

```R
model_full <- lm(rating ~ ., data = attitude) 
model_null <- lm(rating ~ 1, data = attitude)
scope = list(lower = model_null, upper = model_full)
ideal_model = step(model_null, scope = scope, direction = "forward")
summary(ideal_model)
```


# 3.4 Диагностика модели

Ответим на вопрос: можно ли на тех данных, которые имеются, строить модели, которые хотим построить.

```R
# regression diagnostic

data(swiss)
str(swiss)

# relationship between variables
pairs(swiss) # смотрим взаимосвязи между переменными

ggplot(swiss, aes(x = Examination, y = Education)) +
  geom_point()+
  theme(axis.text = element_text(size = 15),
        axis.title = element_text(size = 15, face = "bold"))
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/49.png">

Переменные связаны не очень линейно, с ними будем работать.

Сначала посмотрим на выбросы. Сделаем такой же график, но с линией тренда.

```R
ggplot(swiss, aes(x = Examination, y = Education)) +
  geom_point()+
  theme(axis.text = element_text(size = 15),
        axis.title = element_text(size = 15, face = "bold"))+
  geom_smooth(method = "lm")
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/50.png">

Видно, что некоторые точки довольно сильно отклоняются. На это стоит обратить внимание. 

Теперь посмотрим нормально ли распределены переменные, с которыми мы работаем.

```R
# normality of variables distribution
library(gridExtra)
library(ggplot2)
ggp1 = ggplot(swiss, aes(x=Examination))+
  geom_histogram()

ggp2 = ggplot(swiss, aes(x=Education))+
  geom_histogram()

grid.arrange(ggp1, ggp2, ncol = 2)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/51.png">

Данные об образовании распределены не совсем нормально, скошено влево. В такой ситуации, если все-таки хотим использовать переменную для анализа, нужно ее как-то преобразовать. Для преобразования мы можем взять логирифм, или например, квадратный корень.

Добавим на график логарифм.

```R
library(gridExtra)
library(ggplot2)
ggp1 = ggplot(swiss, aes(x=Examination))+
  geom_histogram()

ggp2 = ggplot(swiss, aes(x=log(Education)))+ # добавили логарифм
  geom_histogram()

grid.arrange(ggp1, ggp2, ncol = 2) 
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/52.png">

**Задача**:*Функция scale() позволяет совершить стандартизацию вектора, то есть делает его среднее значение равным нулю, а стандартное отклонение - единице (Z-преобразование). **Стандартизованный коэффициент регрессии (β)** можно получить, если предикторы и зависимая переменная стандартизованы. Напишите функцию, которая на вход получает dataframe с двумя количественными переменными, а возвращает стандартизованные коэффициенты для регрессионной модели, в которой первая переменная датафрейма выступает в качестве зависимой, а вторая в качестве независимой.*

**!** Z-стандартизация нужна для придания всем атрибутам одинаковых "весов".

```R
beta.coef <- function(x){    
	x <-scale(x)    
	return(lm(x[,1] ~ x[,2])$coefficients)}
```

**Задача**:*Напишите функцию normality.test, которая получает на вход dataframe с количественными переменными, проверяет распределения каждой переменной на нормальность с помощью функции shapiro.test. Функция должна возвращать вектор с значениями p - value, полученного в результате проверки на нормальность каждой переменной. Названия элементов вектора должны совпадать с названиями переменных.*

```R
normality.test  <- function(x){    
	return(sapply(x, FUN =  shapiro.test)['p.value',])}
```

# 3.5 Диагностика модели. Продолжение

```R
ggplot(data = swiss, aes(Examination, Education)) +
  geom_point()+
  geom_smooth()
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/53.png">

Видим, что связь между переменными явно не линейная. Сделаем линейную модель.

```R
lm1 <- lm(Education ~ Examination, swiss)
summary(lm1)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/54.png">

Examination - значимый предиктор, $$ R^{2} = 0.48 $$. Построим модель с предиктором во второй степени.

```R
swiss$Examination_squared <- (swiss$Examination)^2
lm2 <- lm(Education ~ Examination + Examination_squared, swiss)
summary(lm2)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/55.png">

В данном случае эта модель лучше описывает связь. $$ R^{2} $$ больше. Если сделать anova тоже разница будет значима.

Построим графики по предсказанным моделям для двух моделей.

```R
swiss$lm1_fitted <- lm1$fitted
swiss$lm2_fitted <- lm2$fitted
swiss$lm1_resid <- lm1$resid
swiss$lm2_resid <- lm2$resid

ggplot(swiss, aes(x = Examination, y = Education)) +
  geom_point(size=3) +
  geom_line(aes(x = Examination, y = lm1_fitted), col="red", lwd=1)+
  geom_line(aes(x = Examination, y = lm2_fitted), col="blue", lwd=1)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/56.png">

*Другое допущение*: **независимость остатков.** Независимость остатков предполагает, что все наблюдения, которые использованы для модели не сгруппированы по каким-то отдельным категориям. Например, независимость остатков нарушается, если наши наблюдения взяты из двух разных исследований, наблюдения не являются независимыми. 

```R
swiss$obs_number <- seq.int(nrow(swiss))
g1 = ggplot(swiss, aes(x=obs_number, y =lm1_resid)) + # x- номер наблюдения, y - остатки
  geom_point(size=3) + geom_smooth()

g2 = ggplot(swiss, aes(x=obs_number, y =lm2_resid)) + # x- номер наблюдения, y - остатки
  geom_point(size=3) + geom_smooth()

library(gridExtra)
grid.arrange(g1, g2, ncol = 2)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/57.png">

В принципе, нет никаких группировок. В идеале должны были наблюдать линию, вокруг которой равномерно наблюдаются остатки.

**Требование о гомоскедастичности**. Это требование, согласно которому изменчивость остатков должна быть одинаковой на разных уровнях независимой переменной. Т.е. разброс ошибок должен быть одинаков на протяжении всего моделируемого вектора. 

```R
g_r_1 = ggplot(swiss, aes(x = lm1_fitted, y = lm1_resid))+
  geom_point(size=2)
g_r_2 = ggplot(swiss, aes(x = lm2_fitted, y = lm2_resid))+
  geom_point(size=2)
grid.arrange(g_r_1, g_r_2, ncol = 2) 
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/58.png">

В первом случае гомоскедастичность немного нарушается: в начале разброс небольшой, в середине и конце разброс большой.

Во второй модели разброс небольшой, все распределение скошено к началу

**Функция gvlma() из библиотеки gvlma позволяет получить оценку выполнения основных допущений линейной регрессии. В качестве аргумента она принимает объект, в который сохранена модель. Можно задать формулу модели прямо в функции gvlma. Чтобы увидеть основные статистики, нужно выполнить команду summary для объекта, созданного с помощью функции gvlma.**

Например,
```R
x <- gvlma(fit)

# или

x <- gvlma(Y ~ X, data = mydata)

summary(x)
```

**Задача***Загрузите себе прикреплённый к этому степу датасет и постройте регрессию, предсказывающую DV по IV. Установите библиотеку gvlma и проверьте, удовлетворяется ли в этой модели требование гомоскедастичности. Введите в поле ответа p-значение для теста гетероскедастичности.*

```R
install.packages("gvlma")
df = read.csv("https://stepik.org/media/attachments/lesson/12088/homosc.csv")
head(df)
library(gvlma)
x <- gvlma(DV ~ IV, data = df)

summary(x)
```

**Последнее требоавние: Нормальность распределения остатков**

```R
ggplot(swiss, aes(x=lm1_resid))+
  geom_histogram(binwidth = 2, fill="white", col="black")

qqnorm(lm1$residuals)
qqline(lm1$residuals)

shapiro.test(lm1$residuals) # распределение остатков не является нормальным
```

Для второй модели такие же графики можем смотреть. 

**Задача**: *Напишите функцию resid.norm, которая тестирует распределение остатков от модели на нормальность при помощи функции shapiro.test и создает гистограмму при помощи функции ggplot() с красной заливкой "red", если распределение остатков значимо отличается от нормального (p < 0.05), и с зелёной заливкой "green" - если распределение остатков значимо не отличается от нормального. На вход функция получает регрессионную модель. Функция возвращает переменную, в которой сохранен график ggplot.*

```R
resid.norm <- function(fit) {
  p <- shapiro.test(fit$residuals)$p.value
  model_reduced = fit$residuals
  color = ifelse(p < 0.05, "red", "green")
  my_plot = ggplot(fit, aes(x=fit$residuals))+
      geom_histogram(fill=color)
  return(my_plot)
}

fit <- lm(mpg ~ wt, mtcars)
my_plot <- resid.norm(fit)
my_plot
```

**Задача**:*Ещё одной проблемой регрессионных моделей может стать мультиколлинеарность - ситуация, когда предикторы очень сильно коррелируют между собой. Иногда корреляция между двумя предикторами может достигать 1, например, когда два предиктора - это одна и та же переменная, измеренная в разных шкалах (x1 - рост в метрах, x2 - рост в сантиметрах). Проверить данные на мультиколлинеарность можно по графику pairs() и посчитав корреляцию между всеми предикторами c помощью функции cor. Напишите функцию high.corr, которая принимает на вход датасет с произвольным числом количественных переменных и возвращает вектор с именами двух переменных с максимальным абсолютным значением коэффициента корреляции.*

```R
high.corr <- function(x){
  pair_cor = cor(x)
  diag(pair_cor) <- 0
  pair_cor = round(pair_cor, digits = 3)
  mm = abs(pair_cor)
  k = which(mm == max(mm), arr.ind = TRUE)
  return(row.names(k))
}
```

# 3.6 Логистическая регрессия

*Идея метода ЛОГИТ ПРЕОБРАЗОВАНИЯ*: Основное свойство линейных моделей в том, что зависимая переменная должна быть количественной. Логистическая регрессия показывает как влияют как количественные, так и номинативные переменные на номинативную переменную, имеющую две градации.

Событие может случиться или не случиться и какие факторы влияют на исход. 

$$ p_{i}=\beta_{0}+\beta_{1}x_{1,i}+\beta_{2}x_{2,i}+...+\beta_{k}x_{k,i} $$ - зависимая переменная, вероятность того, что произойдет положительный исход.

Основная проблема в том, что в левой чати у нас может быть значение $$ [0; 1] $$, а справа $$ [-\infty; +\infty] $$. Выход из этой ситуации, преобразовать уравнение к виду:

$$ p_{i}=\frac{exp(\beta_{0}+\beta_{1}x_{1,i}+\beta_{2}x_{2,i}+...+\beta_{k}x_{k,i})}{(1+exp(\beta_{0}+\beta_{1}x_{1,i}+\beta_{2}x_{2,i}+...+\beta_{k}x_{k,i}))} $$

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/59.png">

Нужно значение вероятности трансформировать каким-то таким образом, чтобы оно также принимало значение в диапазоне $$ [-\infty; +\infty] $$, как и потенциальные значения правой части уравнения. Начнем с того, что рассчитаем ```Odds```. Тогда нужно вероятность положительного исхода, разделить на вероятность отрицательного исхода. 

Теперь возьмем логарифм от этого значения.

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/60.png">

Теперь посмотрим итоговоую картинку. Это взаимосвязь вероятности $$ p $$ и  $$ logit(p_{i}) $$ (и есть логарифм от Odds).

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/61.png">

Чтобы из $$ logit(p_{i}) $$ обратно получить вероятность, необходимо взять по экспоненте, т.е. сделать обратное действие.

**Рассмотрим пример на R**

Данные: как на факт, что ученик получил или не получил красный диплом hon(Factor) влияют факторы gender(Factor), read(int), write(int), math(int).

```R
library(ggplot2)
my_df <- read.csv('/Users/dima/Documents/learn/stepik_analyz_dannyh_v_R_chast_1/datasets/train.csv', sep=";")
my_df$gender = factor(my_df$gender)
my_df$hon = factor(my_df$hon)
str(my_df)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/62.png">

Построим небольшой график. Посмотрим как на фактор красного диплома влияют баллы по чтению, математике и какого пола ученик.

```R
ggplot(my_df, aes(read, math, col=gender))+
  geom_point()+
  facet_grid(.~hon)+
  theme(axis.text = element_text(size = 3),
        axis.title = element_text(size = 3, face="bold"))
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/63.png">


Тут очевидно, что есть связь по матеше и чтению и будто бы девушкой. С помощью биномиальной регрессии посттроим такую модель, где зависимая будет диплом, а предикторами будут переменные на графике.

```R
fit <- glm(hon ~ read + math + gender, my_df, family="binomial") # простой вариант без взаимодействий. Чаще всего используется биномиальная модель.
summary(fit)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/64.png">

(Intercept). Так как у нас есть gendermale. То (Intercept) это показатель, который будет рассчитан для женщин при условии, что у них предполагается ноль баллов по математике и по чтению. **Наш (Intercept) это $$ logit(p_{i}) $$**.

Переменные read = 0.06677 и math = 0.13907 - оба значимы. Если испытуемый женщина и баллы по математике фиксируемы и неизменяемы, то с единичным увеличением балла по чтению значение логарифма Odds будет увеличиваться на 0.06677.

gendermale = -1.18606. Если у нас баллы по чтению и математике зафиксированы, то сам факт изменения нашей номинативной переменной будет снижать вероятность получения красного диплома на -1.18606.

Если мы возьмем экспоненту, то мы перейдем от Odds к нормальным коэфициентам.

```R
exp(fit$coefficients)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/65.png">

Что теперь делать с предсказанными значениями.

```R
head(predict(object = fit))
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/66.png">

Очевидно, это не вероятности. Это показатели натурального логарифма от Odds.

Чтобы получить результаты в виде вероятностей, необходимо указать аргумент ```type = response```.

```R
head(predict(object = fit, type = "response"))
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/67.png">

Теперь для каждого испытуемого есть вероятность того, что он закончит школу с красным дипломом. Запишем предсказанные значения в новую переменную.

```R
my_df$prob = predict(object = fit, type = "response")
```

Далее на основе вероятностей подбирается порог, при котором ставится "Да/Нет". Для этого строится ROCR-кривая.

Чтобы строить ROCR-кривые юзаем отдельный пакет.

```R
library(ROCR)
pred_fit = prediction(my_df$prob, my_df$hon) # строит объект в который помещаются предсказанные значения и реальные
perf_fit = performance(pred_fit, "tpr", "fpr") # TruePositiveRate, FalsePositiveRate
plot(perf_fit, colorize=T, print.cutoffs.at=seq(0.1,by=0.1))
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/68.png">

TruePositiveRate и FalsePositiveRate - в зависимости от того, какой порог отсечения мы выбираем, возможно различное распределение ошибок. Начем с правого угла, на линии изображены значения порогов, после которого мы будем считать, что событие произойдет с положительным успехом. Например, будем считать, что для человека предсказанная вероятность больше 0.1, точно говорим, что он окончит школу с красным дипломом. Видим, что для 0.1 TruePositiveRate почти 90%. Это означает, что практически в 90%, если человек окончил школу с красным дипломом,а мы предсказали, что он окончит с красным дипломом, наше предсказание будет верным. Однако мы будем допускать очень много ошибок FPR на этом пороге. То есть мы будем говоирть , что человек окончил с красным дипломом, а он на самом деле не окончил. У нас будет очень много людей, для которых мы посчитали, что диплом будет, а они на самом деле не получили. 

Рассмотрим порог 0.8. Будем говорить, что человек окончит школу с красным дипломом только в том случае, если вероятность этого события больше чем 0.8. У нас будет очень низкий FalsePositiveRate. Но теперь будет не очень много TruePositiveRate.

В функции performance мы можем посмотреть площадь под кривой, хороший показатель эффективности классификатора. 

```R
auc <- performance(pred_fit, measure = "auc")
auc # num 0.87
str(auc)
```

Довольно неплохой показатель для классификатора.

### Какой отобрать порог отсечения

Рассмотрим *специфичность* и *чувствительность* классификатора.

```R
perf3 <- performance(pred_fit, x.measure = "cutoff", measure = "spec") # Специфичность. x-значение порога, y-специфичность
perf4 <- performance(pred_fit, x.measure = "cutoff", measure = "sens") # Чувствительность y - чувствительность.
perf5 <- performance(pred_fit, x.measure = "cutoff", measure = "acc") # Соотношение значением от порога и общей эффективностью классификатора.
plot(perf3, col="red", lwd=2)
plot(add=T, perf4, col="green", lwd=2) 
plot(add=T, perf5, lwd=2)
legend(x=0.6, y=0.3, c("spec", "sens", "accur"),
       lty = 1, col = c('red', 'green', 'black'), bty = 'n', cex = 1, lwd = 2)
abline(v=0.225, lwd=2)
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/69.png">

Порог отсечения разумно взять на пересечении. В данном случае 0.2

*Специфичность* - насколько хорошо нам удается правильно предсказывать отрицательный исход события. Мы говорим, что он **не получил** и он действительно **не получил**. Если порог принятия регения Cutoff очень большой, 0.8, например, то мы мало будем ошибаться на минус минус. 

*Чувствительность* - как хорошо нам удается предсказывать положительный исход события. Мы сказала, что человек получил и он получил.

Теперь создадим у испытуемых новую переменную pred_respond. Это будет не просто вероятность, а да/нет. И построим график.

```R
my_df$pred_resp <- factor(ifelse(my_df$prob > 0.225, 1, 0), labels = c("N", "Y")) # предсказанная певроятность
my_df$correct <- ifelse(my_df$pred_resp == my_df$hon, 1, 0) # правильно ли предсказали
View(my_df)
ggplot(my_df, aes(prob, fill=factor(correct)))+
  geom_dotplot()+
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=10, face="bold"))
```

<img src="/assets/img/2020-01-02-konspekt-analyz-dannyh-r/70.png">

Красные - те, что мы предсказали неправильно.







# 3.7 Экспорт результатов



```R
```


```R
```
s





















