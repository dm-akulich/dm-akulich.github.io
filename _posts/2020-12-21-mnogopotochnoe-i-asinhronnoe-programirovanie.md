---
layout: post
title: Многопоточное и асинхронное программирование в Python (Crs. Погружение в Python)
comments: False
category: python
tags:
---

# 1. Процессы и потоки

**Выполнение асинхронных программ их выполнение в процессах и потоках.**

## 1.1. Процесс и его характеристики

### Что такое процесс

Процесс — это программа, которая запущена в оперативной памяти компьютера.

Процесс — это набор инструкций, которые выполняются последовательно.

### Процессы в ОС

Характеристики процесса:

- Идентификатор процесса, PID
- Объем оперативной памяти
- Стек - Стек используется для вызова функций, для создания локальных переменных у этих функций.
- Список открытых файлов
- Ввод/вывод

В общем случае процессы в ОС выполняются последовательно. Планировщик ОС выделяет небольшие кванты времени каждому процессу, последовательно выполняет один процесс и переключается на выполнение следующего процесса.

### Запуск процесса в Python

```python
import time
import os

pid = os.getpid() # получим id процесса

while True:
    print(pid, time.time())
    time.sleep(2)
```

## 1.2. Создание процессов

Как создать дочерний процесс в Python, как работает системный вызов ```fork```, примеры создания процессов при помощи модуля ```multiprocessing```.

Процесс в операционной системе создается при помощи системного вызова ```fork```. Рассмотрим программу, которая создает дочерний процесс при помощи системного вызова ```fork```.

```python
# Создание процесса на Python
import time
import os

pid = os.fork()

if pid == 0:
    # дочерний процесс
    while True:
        print("Child process:", os.getpid())
        time.sleep(5)
else:
    # родительский процесс
    print("Parent process:", os.getpid())
    os.wait()

>>> "Parent process: 1252"
>>> "Child process: 1253"
>>> "Child process: 1253"
```

Системный **вызов ```fork``` создает точную копию родительского процесса**. Это означает, что вся память, все файловые дискрипторы и все ресурсы, которые были доступны в родительском процессе, будут целиком и полностью скопированы в дочернем процессе. То есть после того, как системный вызов ```fork``` отработал, с этого момента у нас два процесса в операционной системе. Единственное отличие заключается в том, что системный вызов ```fork``` в родительский процесс вернет ```pid``` дочернего процесса, а в дочернем процессе переменная ```pid``` будет равна нулю. Код, который находится за веткой ```else```, будет исполнен в родительском процессе. Итак, в родительском процессе мы вызываем системный вызов ```os.wait```, это еще один дополнительный системный вызов и он позволяет нам дожидаться завершения дочернего процесса созданного. А в дочернем процессе в бесконечном цикле выводим ```pid``` нашего процесса, который создали, и спим пять секунд.

Запустим код в терминале

<img src="/assets/img/2020-12-21-mnogopotochnoe-i-asinhronnoe-programirovanie/1.png">

Посмотрим на эти процессы в системе

```bash
ps uax | grep <filename.py>
```

В данном случае ```ps uax | grep process2.py```.

<img src="/assets/img/2020-12-21-mnogopotochnoe-i-asinhronnoe-programirovanie/2.png">

Мы видим процессы с pid'ами **1419** и **1418**. Это и есть созданные процессы.

Посмотрим, что процессы делают с помощью ```sudo strace -p 1418```

Итак, мы видим, что наш созданный дочерний процесс делает системный вызов write и выводит информацию в стандартный поток вывода.


**fork - создает точную копию текущего процесса**

Давайте остановимся немного еще раз на памяти в родительском и дочернем процессе и рассмотрим пример. Итак, у нас есть программа, мы объявили в ней переменную **foo**, присвоили ей значение **"bar"** и делаем системный вызов **fork**. 

```python
import os

foo = "bar"

if os.fork() == 0:
    # дочерний процесс
    foo = "baz"
    print("Child:", foo)
else:
    # родительский процесс
    print("Parent:", foo)
    os.wait()

>>> Parent: bar
>>> Child: baz
```

После того, как отработал системный вызов fork, как я уже говорил, вся память целиком и полностью будет скопирована из родительского процесса в дочерний. То есть у нас переменная foo будет точно так же доступна в дочернем процессе. И у нее будет значение "bar", так как память была скопирована. Но если мы изменим значение foo в дочернем процессе, это никак не повлияет на родительский процесс, на переменную foo, которая была объявлена в родительском процессе. Если мы выведем в родительском процессе ее значение, то оно будет равно значению "bar". Итак, память целиком и полностью копируется. Если ее рассматривать, то это разные памяти. У дочернего процесса своя, у родительского процесса своя. То же самое относится и к файловым дискрипторам. 

В Python'е используют модуль **multiprocessing** для создания процессов.

```python
# создание процесса
from multiprocessing import Process

def f(name):
    print("hello", name)

p = Process(target=f, args=("Bob",))

if __name__ == '__main__':
    p.start()
    p.join()

>>> 'hello Bob'
```

Альтернативный метод создания процесса

```python
from multiprocessing import Process

class PrintProcess(Process):
    def __init__(self, name):
        super().__init__()
        self.name = name

    def run(self):
        print("hello", self.name)

if __name__ == '__main__':
    p = PrintProcess("Mike")
    p.start()
    p.join() # будем ждать пока завершиться в основном потоке

>>> 'hello Mike'
```

Для того, чтобы запустить процесс на Python необходимо импортировать класс Process из модуля multiprocessing, создать объект класса Process, передать ему в конструктор функцию, которую мы хотим исполнить в отдельном дочернем процессе и аргументы этой функции. После того, как мы создали объект, никакого процесса создано не будет. Процесс будет создан тогда, когда мы вызовем метод start нашего объекта. Вот здесь, внутри метода start, будет вызван системный вызов fork и исполнена наша функция f в отдельном процессе. Очень важно ожидать завершения всех созданных дочерних процессов. Для этого можно воспользоваться удобной функцией join для нашего объекта **p**. Как мы видим, системные вызовы fork и wait спрятаны внутри красивых оберток. Вообще, не в каждой операционной системе есть системный вызов fork и поэтому в multiprocessing все аккуратно сделано за вас. И не надо за это беспокоиться. Просто используем модуль multiprocessing для создания собственных дочерних процессов. Если мы исполним этот пример, то в стандартном выводе мы увидим строчку hello Bob.


## 1.3. Создание потоков

Поток целиком и полностью напоминает процесс. Он имеет свою последовательность инструкций для исполнения, у каждого потока есть свой собственный стек, но **все потоки выполняются в рамках одного процесса**. Этим они отличаются. Если мы говорили о процессах и у каждого процесса были свои ресурсы и память, то **все созданные потоки разделяют память процесса и все его ресурсы**. Управлением и выполнением потоков занимается операционная система. Но в Python есть свои ограничения для потоков. 

```python
# создание потока
from threading import Thread

def f(name):
    print("hello", name)

if __name__ == '__main__':
    th = Thread(target=f, args=("Bob",))
    th.start()
    th.join()

>>>> "hello Bob"
```

```python
# альтернативный метод создания потока
from threading import Thread

class PrintThread(Thread):
    def __init__(self, name):
        super().__init__()
        self.name = name

    def run(self):
        print("hello", self.name)

if __name__ == '__main__':
    th = PrintThread("Mike")
    th.start()
    th.join()

>>>> "hello Mike"
```

В Python3 появился очень удобный класс для создания пула потоков. Называется он ```ThreadPoolExecutor``` модуль ```concurrent.futures```. Предположим, у нас есть некий массив чисел, и нам нужно ограничить количество потоков и рассчитать квадраты чисел для этого массива. Для этого можно использовать контекстный менеджер, указать в нем вызов ```ThreadPoolExecutor``` с параметром ```max_workers```, который как раз отвечает за максимальное количество потоков, которые будут созданы в этом блоке ```with```. Не нужно заботиться о завершении потоков. Нужное количество потоков будет создано автоматически, и при завершении контекстного менеджера будет вызвана функция shutdown, которая дождется завершения всех созданных потоков. Основная функция у этого ```ThreadPoolExecutor``` — это метод ```submit```. Метод ```submit``` создает объект класса ```concurrent.futures``` ```Future``` — это такой объект, который еще не завершился, но выполняется и будет завершен в будущем. При помощи удобного метода ```as_completed``` из этого модуля ```concurrent.futures``` мы можем дождаться завершения всех наших объектов и получить результаты по мере завершения всех этих потоков, созданных нашим ```Executor```. Если мы запустим пример, мы опять получим вывод, похожий на слайд. Итак, в этом видео мы обсудили, что такое поток. Мы рассмотрели особенности создания потоков при помощи модуля ```threading```. Ключевое отличие потоков от процессов состоит в том, что они разделяют память и все ресурсы процессов, в рамках которого они запущены. 

```python
# Пул потоков, concurrent.futures.Future

from concurrent.futures import ThreadPoolExecutor, as_completed

def f(a):
    return a * a

# .shutdown() in exit # будет вызвана сама
with ThreadPoolExecutor(max_workers=3) as pool:
    results = [pool.submit(f, i) for i in range(10)]

    for future in as_completed(results):
        print(future.result())
```

## 1.4. Синхронизация потоков

- очереди
- блокировки
- условные переменные

Если мы запустим несколько потоков для решения своей задачи, то рано или поздно придётся обмениваться данными между потоками. Это обсудим.

Разберёмся, **как можно использовать модуль queue** и **очереди** для обмена данными между потоками. Использование очередей выглядит достаточно простым. На примере мы создаём объект типа очередь с максимальным размером 5. Для помещения элементов в очередь необходимо использовать метод ```put``` для объекта ```queue```. Eсли в очереди будет уже пять элементов, то вызов метода ```put``` заблокирует выполнение потока, который вызвал этот метод, и будет ждать, пока не появится в очереди свободное место. 

```python
# Очереди, модуль queue
from queue import Queue
from threading import Thread

def worker(q, n): # функция будет выполняться в двух потоках параллельно
    while True:
        item = q.get()
        if item is None:
            break
        print("process data:", n, item)

q = Queue(5)
th1 = Thread(target=worker, args=(q, 1))
th2 = Thread(target=worker, args=(q, 2))
th1.start(); th2.start()

for i in range(50):
    q.put(i)

q.put(None); q.put(None)
th1.join(); th2.join()

>>> "process data:process data: 1 0"
>>> "process data: 1 2"
>>> "process data: 1 3"
>>> "process data: 1 4"
>>> "process data: 2 6"
>>> "process data: 2 7"
>>> "process data: 2 8"
```

Для обработки сообщений этой очереди мы создаём пару потоков — это объекты класса Thread. Передаём в этот объект функцию ```worker```, и этой функции мы передаём нашу очередь. Итак, наша функция ```worker``` будет выполняться в двух потоках независимых. Они будут выполняться параллельно. Каждый поток в бесконечном цикле будет получать сообщение из очереди при помощи вызова метода ```get``` у объекта ```q```. 

Потоки выполнили этот код параллельно, и мы видим то цифру 2, то цифру 1 в нашем стандартном выводе. Большое внимание нужно уделить правильному завершению потока. **С точки зрения процесса, ресурсами владеет процесс, то есть выделенная память или открытый файл — ими владеет процесс.** Но процесс ничего не знает о том, что делает с этими ресурсами поток.

Если поток завершить аварийно, то файл может остаться незакрытым, блокировка может остаться невысвобожденной, и теоретически это может привести к непредвиденным последствиям. 

Важно делать это правильно в функции самого потока. На приведённом примере в очередь помещается специальное значение None, и функция потока при проверке условия завершает свою работу. Код с использованием очередей выглядит достаточно просто, и предпочтительнее использовать очереди при разработке многопоточных программ.

**Блокировки**

Иногда приходится использовать блокировки. Блокировки как минимум замедляют работу программы. Тем не менее, иногда их нужно применять. 

Предположим, у нас есть класс Точка, и у класса Точка есть координаты **x** и **y**. Также у этого класса есть метод ```get```, который возвращает эти координаты, и метод ```set```, который задаёт новые координаты. Предположим, что мы создали объект класса Точка и используем этот объект в большом количестве потоков. Эти потоки, некоторые вызывают метод ```get```, некоторые вызывают метод ```set```. Если бы не было блокировок, то может возникнуть такая ситуация, когда один поток изменил значение переменной **x** или координаты **x**, а другой поток в это время вернул координаты **x** и **y**. Мы получили неконсистентное состояние объекта, когда у него частично одна координата изменена, а вторая нет. Для того чтобы избежать подобных ситуаций, и нужны блокировки. 

```python
import threading

class Point(object):
    def __init__(self, x, y):
        self.mutex = threading.RLock() # создание объекта блокировки
        self.set(x, y)

    def get(self):
        with self.mutex: # с помощью контекстного менеджера захватываем блокировку
            return (self.x, self.y)

    def set(self, x, y):
        with self.mutex:
            self.x = x
            self.y = y

# use in threads
my_point = Point(10, 20)
my_point.set(15, 10)
my_point.get()
```

Этот код гарантирует что если объект класса **Point** будет использоваться в разных потоках,
то изменение **x** и **y** будет всегда атомарным.

Работает все это так: - при вызове метода берем блокировку через ```with self._mutex```. Весь код внутри ```with``` блока будет выполнятся только в одном потоке.

Другими словами, если два разных потока вызовут **.get** то пока первый поток не выйдет из блока второй будет его ждать - и только потом продолжит выполнение.

Зачем это все нужно? Координаты нужно менять одновременно - ведь точка это атомарный объект. Если позволить одному потоку поменять **x**, а другой в это же время поправит **y** логика алгоритма может сломаться.

**Еще про блокировки**

Ещё один вариант применения блокировок. Их можно использовать без контекстного менеджера. Выглядит это тоже довольно-таки просто. Мы создаём объекты класса **RLock** и затем вызываем методы ```acquire``` — это получить или захватить блокировку и метод release для того, чтобы высвободить её. Если мы запустим подобный код в большом количестве процессов, то рано или поздно это приведёт к ситуации, которая называется deadlock. Дело в том, что мы освобождаем в неправильной последовательности блокировки. Нужно учитывать это в своих программах и отдавать предпочтение использованию контекстного менеджера при работе с блокировками. Также в Python существует ещё и объект класса **Lock**, а не **RLock**, но предпочтительнее использовать объекты **RLock**. Они позволяют в одном потоке получить блокировку дважды. 

```python
# Синхронизация потоков, блокировки

import threading

a = threading.RLock()
b = threading.RLock()

def foo():
    try:
        a.acquire() # получить блокировку
        b.acquire()
    finally:
        a.release() # высвободить блокировку
        b.release()
```

В Python существует ещё один механизм для синхронизации потоков. Он называется "условные переменные". Давайте рассмотрим класс Очередь. Это очередь, с которой нужно будет работать в большом количестве потоков. У неё есть операции ```put``` и ```get```, и, конечно же, у неё есть какой-то размер. Очередь не должна расти больше заданного размера. Если мы выполним операцию put, а в очереди уже достаточно большое количество элементов, то нам необходимо ждать пока это количество уменьшится. Вопрос — сколько ждать? Неизвестно. Ответа на этот вопрос мы не получим. Для решения подобной задачи можно использовать условные переменные. Условные переменные в конструктор получает объект блокировки. Он есть по умолчанию, но если у нас эти переменные взаимозависимые, то необходимо использовать общую блокировку (```threading.Condition(self._mutex)```). И при помощи этих условных переменных очень легко и удобно ожидать событий при помощи вызова wait и оповещать все потоки, которые сейчас ждут наступления этого события. Таким образом, очень легко и удобно можно реализовать очередь в Python, которая работает в многопоточной программе. 

```python
# Синхронизация потоков, условные переменные

class Queue(object):
    def __init__(self, size=5):
        self._size = size
        self._queue = []
        self._mutex = threading.RLock()
        self._empty = threading.Condition(self._mutex)
        self._full = threading.Condition(self._mutex)
    
    def put(self, val):
        with self._full:
            while len(self._queue) >= self._size:
                self._full.wait()
            
            self._queue.append(val)
            self._empty.notify()

    def get(self):
        with self._empty:
            while len(self._queue) == 0:
                self._empty.wait()
            
            ret = self._queue.pop(0)
            self._full.notify()
            return ret
```

## 1.5. Глобальная блокировка интерпретатора, GIL

- Что такое Global Interpreter Lock?
- Зачем нужен GIL?
- GIL и системные вызовы

 Для более глубокого понимания того как работают потоки нужно иметь общее представление зачем нужен GIL и как он устроен.

GIL защищает память интерпретатора от повреждений и делает операции атомарными.

Поток, владеющий GIL, не отдает его пока об этом не попросят.
Потоки засыпают на 5 мс. для ожидания GIL. Сам GIL устроен как обычная нерекурсивная блокировка. Эта же структура лежит в основе threading.Lock. 

Когда Python делает системный вызов или вызов из внешней библиотеки он отключает механизм GIL. После того как функция вернет управление снова включает его.

Т.е. потоки при своем выполнении так или иначе вынуждены получать GIL. Именно поэтому многопоточные программы, требующие больших вычислений, могут выполняться медленней чем однопоточные.

**GIL не позволяет одновременно двум потокам выполняться на одном ядре процессора, даже если этих ядер у процессора несколько. Тем не менее, GIL в первую очередь предназначен для защиты памяти интерпретатора от разрушений и делает все операции с памятью атомарными.**

```python
# cpu bound programm

from threading import Thread
import time

def count(n):
    while n > 0:
        n -= 1

# series run
t0 = time.time()
count(100_000_000)
count(100_000_000)
print(time.time() - t0)

# parallel run
t0 = time.time()
th1 = Thread(target=count, args=(100_000_000,))
th2 = Thread(target=count, args=(100_000_000,))

th1.start(); th2.start()
th1.join(); th2.join()
print(time.time() - t0)

>>> "18.148294925689697"
>>> "19.478510856628418" 
```

**Что он потребляет?**

<img src="/assets/img/2020-12-21-mnogopotochnoe-i-asinhronnoe-programirovanie/3.png">

Просто вызов функции 2 раза занял **18.148294925689697**

Параллельное выполнение с помощью потоков заняло **19.478510856628418**

Видим, что параллельное выполнение при помощи потоков заняло больше времени.
Как же так? Тогда зачем нужны потоки вообще, и почему так происходит?

Всё дело как раз здесь в глобальной блокировке интерпретатора. Дело в том, что потоки при выполнении своего кода каждый раз получают блокировку интерпретатора. Если у нас задача CPU bound, так называют задачи, которые потребляют только процессор, то код, написанный с использованием тредов в Python, будет неэффективным.

Он будет работать медленнее, чем код, который запущен последовательно. Тем не менее, если мы код нашей функции заменим, например, на задачу, которая требует операции ввода-вывода, то мы заметим большой прирост в итоговом времени выполнения, если сравнивать параллельное выполнение и выполнение в тредах. Если изобразить схематично, как выполняется поток, то выглядит это следующим образом. У нас есть поток, в котором выполняется наш Python код, и каждый раз Python интерпретатор пробует получить глобальную блокировку интерпретатора.

Если Python выполняет операцию ввода-вывода или системный вызов, то он эту блокировку снимает, и далее выполнение происходит без блокировки.
Поэтому если у нас таких будет потоков много, все задачи с вводом-выводом, с ожиданием завершения для операции ввода-вывода будут очень хорошо параллелиться.

Это нужно учитывать в своих задачах, если вы будете применять потоки или процессы. **GIL внутри реализован как обычная нерекурсивная блокировка, или объект класса threading lock.**

Все потоки спят пять миллисекунд в ожидании получения блокировки, и в Python 3, если работает один главный поток, то он не требует освобождения этой глобальной блокировки интерпретатора.
Итак, в этом видео мы обсудили, что такое GIL и какое он отношение имеет к потокам в Python. Так, Python потоки — это обычные потоки, или POSIX threads (обычные потоки), но с ограничениями в виде глобальной блокировки интерпретатора.

Все потоки выполняются с захватом GIL, но системные вызовы и операции ввода-вывода, для них GIL не нужен.

Итак, мы рассмотрели вопросы про то, как работают потоки и процессы, и в следующих видео мы рассмотрим, как устроены сокеты и как работать с сетью с применением полученных знаний о потоках и процессах. 

```
# как выполняется поток?

a      r      a            r              a          r    a
  run  |------|    run     |--------------|   run    |----| run
------>|  IO  |----------->|      IO      |--------->| IO |----->
       |------|            |--------------|          |----|
a      r      a            r              a          r    a

a - acquire GIL
r - release GIL
```

# 2. Работа с сетью, сокеты

**То, как устроены socket'ы, и выполнение различных сетевых запросов.**

## 2.1. Сокеты, клиент-сервер

Сокеты — это **кросс-платформенный механизм для обмена данными между отдельными процессами**. Эти процессы могут работать на разных серверах, они могут быть написаны на разных языках, и, прежде всего, программа на Python, которая использует механизм сокетов, она **осуществляет системные вызовы и взаимодействие с ядром операционной системы**.

Как правило, для организации сетевого взаимодействия нужен сервер, который изначально создает некое соединение и начинает «слушать» все запросы, которые поступают в него и программа-клиент, которая присоединяется к серверу и отправляет ему нужные данные. 

При помощи сокетов можно организовать взаимодействие между процессами,
работающим на разных серверах.

Сокет в ОС Linux - это объект уровня ядра.

Т.е. **python процесс, при создании сокета и вызова функций осуществяет системные вызовы**. Ядро ОС возвращает результаты работы системных вызовов python процессу.

Пример серверной программы. 

**Сервер**

```python
# создание сокета, сервер

import socket

# https://docs.python.org/3/library/socket.html
sock=socket.socket(socket.AF_INET, socket.SOCK_STREAM) # должны создать объект типа сокет.
sock.bind(("127.0.0.1", 10001))   # max port 65535
sock.listen(socket.SOMAXCONN) # начать слушать соединения

conn, addr = sock.accept()
while True:
    data = conn.recv(1024)
    if not data:
        break
    # process data
    print(data.decode("utf8"))

conn.close()
sock.close()
```
Для того чтобы создать сокет, мы должны импортировать модуль socket. Далее мы должны создать объект типа socket из модуля socket. В него необходимо передать некоторые параметры. В данном случае это некоторое семейство — мы используем address family, мы используем конкретную константу AF_INET, а также тип сокета. В данном примере мы используем потоковый сокет. Полную информацию по типам сокетов, по типам address family можно посмотреть в документации на Python, либо в документации про то, как устроена сеть в операционной системе Linux. Итак, мы создали объект socket — это потоковый сокет. Далее мы должны вызвать метод bind. В метод bind мы должны передать некую адресную пару — это host, в данном случае мы передаем 127.0.0.1, и порт 127.0.0.1 будет означать, что наш сервер будет слушать все входящие соединения только локально на одной машине. Если мы укажем пустую строчку, либо адрес 0.0.0.0, то наш сервер будет слушать входящие соединения со всех интерфейсов. Порт — это некая целочисленная константа, существуют некоторые зарезервированные порты, например, 80-й порт, обычно на нем работает HTTP-сервер, 43-й порт, 443-й порт. Как правило, порты с номерами до 2,000 являются системными, и мы должны использовать адреса больше значений 2,000, но максимальное значение для порта — это 65,535. Итак, системный вызов bind зарегистрировал нашу адресную пару в операционной системе. Двигаемся дальше.

Далее, для того чтобы начать принимать соединения, мы должны вызвать метод listen. У метода listen есть необязательный параметр — это так называемый backlog, или размер очереди входящих соединений, которые еще не обработаны, для которых не был вызван метод accept. Если наш сервер будет не успевать принимать входящие соединения, то все эти соединения будут копиться в этой очереди, и если она превысит это максимальное значение, то операционная система выдаст ошибку ConnectionRefused для клиентской программы. Двигаемся дальше. Мы создали сокет, зарегистрировали адресную пару, вызвали метод listen. Далее мы должны вызвать метод accept, для того чтобы начать принимать входящее клиентское соединение. Системный вызов accept по умолчанию заблокируется, до тех пор, пока не появится клиентское соединение. Итак, если клиент вызовет метод connect, то наш метод accept вернет нам объект, который будет являться полнодуплексным каналом. У этого объекта будут доступны методы записи в этот канал и методы чтения. В нашем примере мы в бесконечном цикле будем вызывать чтение из нашего полнодуплексного канала. Если мы ничего не прочитали, это будет означать, что клиент закрыл соединение и нам необходимо тоже прекратить работу. В качестве обработки наших данных, которые мы прочитали с канала, мы просто выводим эти данные в консоль. После того как мы закончили работу с нашим клиентом, мы вызываем метод close для нашего объекта, который представляет собой полнодуплексный канал, и также закрываем сокет, который слушает новые соединения со стороны клиента. 

**Клиент**

```python
# создание сокета, клиент

import socket

sock = socket.socket()
sock.connect(("127.0.0.1", 10001))
sock.sendall("ping".encode("utf8"))
sock.close()

# более короткая запись

sock = socket.create_connection(("127.0.0.1", 10001))
sock.sendall("ping".encode("utf8"))
sock.close()
```

Код на стороне клиента. Для того чтобы установить соединение с сервером, мы должны создать объект типа socket.socket. По умолчанию создается потоковый сокет с семейством **address family AF_INET**. После этого мы должны вызвать метод **connect**. **Connect** заблокируется до тех пор, пока сервер со своей стороны не вызовет метод accept. После того как системный вызов **connect** отработал, наш сокет готов к работе, и для него можно вызывать методы **send**, **sendall** или **recv**, для того чтобы получать данные с сервера. То есть, по сути, мы получили такой же полнодуплексный канал, с которым можно работать, отправлять и получать данные. После того как мы завершили работу с нашим клиентским сокетом, необходимо вызвать метод **close**.

В Python существует более короткая запись для создания клиентского сокета — это вызов метода модуля **socket** **create_connection**. В **create_connection** мы передаем адресную пару, необязательный **timeout**. Про **timeout** мы еще с вами будем говорить в следующих видео. Этот вызов возвращает нам проключенное соединение, готовое для того, чтобы делать отправку или прием данных. Давайте попробуем запустить наш код и посмотрим, как он работает на самом деле. Нам потребуется код нашего сервера.

**Соединим сервер с клиентом**

1. Запускаем сервер

<img src="/assets/img/2020-12-21-mnogopotochnoe-i-asinhronnoe-programirovanie/4.png">

2. Отправим с клиента на сервер строку (строка должны быть в байтах)

<img src="/assets/img/2020-12-21-mnogopotochnoe-i-asinhronnoe-programirovanie/5.png">

3. Пришло

<img src="/assets/img/2020-12-21-mnogopotochnoe-i-asinhronnoe-programirovanie/6.png">

Для этого нам нужно вызвать метод **close**.
Итак, мы закрыли соединение со стороны клиента. Наш сервер прекратил цикл **while** и закрыл соединение, и наша программа со стороны сервера завершила свою работу. Как я уже говорил, **socket** — это кроссплатформенный механизм и необязательно программа-клиент и сервер должны быть написаны на одном и том же языке.

**Некоторые ключевые моменты**

В частности, это вызовы методов **close** для наших объектов, для соединения, на котором мы акцептим новые соединения клиентов, а также для объекта connection, который является полнодуплексным каналом. Наш код носит обучающий характер, и в нём нет обработки ошибок. Как правило, сетевые программы настоящие, они выглядят более сложно, в них, естественно обрабатываются ошибки, и одним из требований к сетевым программам является то, что необходимо правильно и грамотно завершать работу с созданными **connect'ами** и со всеми открытыми сокетами, тем самым контролируя ресурсы процесса, в котором всё это работает. В Python существует более удобный механизм для работы с сокетами в виде контекстных менеджеров. Давайте рассмотрим пример, в котором мы выполняем те же самые задачи, но используем контекстный менеджер. Итак, мы используем на стороне сервера конструкцию with socket.socket, создаём наш объект типа socket, вызываем методы **bind**, **listen**. Затем в бесконечном цикле вызываем **accept** и получаем новые соединения от клиентов. Для полученного объекта мы опять используем контекстный менеджер и мы не заботимся о вызовах метода **close**. После того как контекстный менеджер завершит свою работу, он автоматически вызовет метод **close** для нужных нам объектов. Это очень удобно, и это позволяет допускать вам меньшее количество ошибок при работе с сокетами. На стороне клиента мы используем снова контекстный менеджер для вызова **socket.create.connection** и опять не заботимся о вызове метода **close**. Предпочтительнее работать с контекстными менеджерами при написании клиент-серверных программ на языке Python. Итак, в этом видео мы обсудили, что такое сокеты, как они устроены, мы попробовали написать свою первую сетевую программу типа клиент-сервер, попробовали поотправлять данные с клиента на сервер, и мы обсудили роль контекстных менеджеров в языке Python для написания сетевых программ. В следующем видео мы продолжим с вами знакомство с сетевыми программами и углубимся в детали их работы. 

```python
# создание сокета, контекстный менеджер
# сервер
import socket

with socket.socket() as sock:
    sock.bind(("", 10001))
    sock.listen()
    
    while True:
        conn, addr = sock.accept()
        with conn:
            while True:
                data = conn.recv(1024)
                if not data:
                    break
                print(data.decode("utf8"))

# клиент
import socket

with socket.create_connection(("127.0.0.1", 10001)) as sock:
    sock.sendall("ping".encode("utf8"))
```

## 2.2. Таймауты и обработка сетевых ошибок

```python
# создание сокета, таймауты и обработка ошибок
# сервер
import socket

with socket.socket() as sock:
    sock.bind(("", 10001))
    sock.listen()
    while True:
        conn, addr = sock.accept()
        conn.settimeout(5)  # timeout := None|0|gt 0 
        with conn:
            while True:
                try:
                    data = conn.recv(1024)
                except socket.timeout:
                    print("close connection by timeout")
                    break
                
                if not data:
                    break
                print(data.decode("utf8"))
```

Мы создаем сокет при помощи контекстного менеджера. Вызываем методы **bind** и **listen**. Затем в бесконечном цикле вызываем метод accept. И слушаем наш сокет, получаем новое соединение. После того, как мы получили это соединение, мы вызываем метод **settimeout** для нашего объекта. И передаем туда значение **"5"**. По умолчанию все вызовы для этого объекта соединения, например вызов **recv**, или вызов send, они будут заблокированы до тех пор, пока данные на другой стороне кто-то не сможет прочитать или записать. По умолчанию таймаута нет. Мы можем передать значение None и это как раз будет значением по умолчанию. Если мы передадим **timeout = 0**, это будет означать немного другое. Это переведет наш сокет в неблокирующий режим. Про неблокирующий режим мы будем говорить чуть позднее. Также можно задать свой таймаут. Итак, если мы задали таймаут и вызвали метод **recv** у нашего сокета, и в этот сокет не поступило данных в течении пяти секунд, то будет сгенерировано исключение **socket.timeout**. Его можно перехватить и обработать. Как обрабатывать, вам нужно решать самим. Можно, например, закрывать соединение. Можно, например, если мы пишем в это соединение, повторно отправлять туда данные, или делать **retry**. Все зависит от требований к вашей программе. Но если вы работаете с таймаутами, вам нужно быть готовым к этому как на стороне сервера, так и на стороне клиента. Итак, в данном случае мы просто закрываем соединение. Метод **close** будет вызван автоматически, когда будет выход из нашего контекстного менеджера.


```python
# создание сокета, таймауты и обработка ошибок
# клиент
import socket

with socket.create_connection(("127.0.0.1", 10001), 5) as sock:
    # set socket read timeout
    sock.settimeout(2)
    try:
        sock.sendall("ping".encode("utf8"))
    except socket.timeout:
        print("send data timeout")
    except socket.error as ex:
        print("send data error:", ex)
```

Рассмотрим код на клиенте. На клиенте существует так называемый connect **timeout** и **socket read timeout**. Их еще называют именно так. **connect timeout** мы задаем в методе **create_connection** и этот таймаут будет распространяться только на установку соединения с нашим сервером. То есть если в течении 5 секунд наш сервер не смог подключить соединение, и наше соединение не было установлено, то возникнет исключение **socket.timeout** и нам на стороне клиента нужно будет его обработать. То есть сделать переподключение, подождать некоторое время и попробовать создать это соединение снова. В данном случае эти таймауты могут немножко различаться. Так как, например, на установку соединения может требоваться немного больше времени, поэтому этот таймаут может быть задан большим. После того, как соединение установлено, мы можем задать таймаут на все операции с нашим сокетом. Например, если мы не смогли записать данные, или прочитать данные с сокета, то сделать соответствующую обработку. Также может возникнуть любое другое исключение и его точно так же нужно будет обработать. Базовый класс для обработки исключений модуля **socket** - это **socket.error**.

Давайте попробуем запустить наш пример и посмотрим, как работают таймауты в консоли.

1. Запусаем сервер, в котором создаем сокет с задержкой

2. запускаем клиент

<img src="/assets/img/2020-12-21-mnogopotochnoe-i-asinhronnoe-programirovanie/7.png">

3. Нужно успеть подключиться за 5 секунд.

## 2.3. Обработка нескольких соединений

Рассмотрим пример с множественной обработкой соединений на стороне сервера

Какие преимущества дает создание процессов?
Можно утилизировать все ядра CPU.
Вызов fork - это слишком тяжелая операция.
Иногда дороже сделать fork, чем обработать сам запрос.
Для процессов будет большой расход памяти.

Использование потоков ограничено GIL и одним процессом.
Мы можем исчерпать 100% CPU на одном ядре, программа будет работать неэффективно.

Как поведет себя ОС при большом кол-ве процессов или потоков?

```python
# обработка нескольких соединений одновременно, потоки

import socket
import threading

def process_request(conn, addr):
    print("connected client:", addr)
    with conn:
        while True:
            data = conn.recv(1024)
            if not data:
                break
            print(data.decode("utf8"))

with socket.socket() as sock:
    sock.bind(("", 10001))
    sock.listen()
    while True:
        conn, addr = sock.accept()
        th = threading.Thread(target=process_request, args=(conn, addr))
        th.start()
        
```



# 3. Асинхронное программирование

**То, как устроены генераторы и корутины в языке Python и рассмотрим примеры работы с frameworkom asyncio**

## 3.1. Исполнение кода в одном потоке, модуль select
## 3.2. Итераторы и генераторы. В чем разница?
## 3.3. Генераторы и сопрограммы
## 3.4. Asyncio. Начало

Asyncio отвечает за неблокирующий ввод/вывод, на этом фреймворке можно написать сервис, который работает с десятками тысяч сое динений одновременно. 

В основе работы этого фреймворка лежат генераторы и корутины.

Пример:

```python
# asyncio_1.py
# asyncio, Hello World
import asyncio

@asyncio.coroutine
def hello_world():
    while True:
        print("Hello World!")
        yield from asyncio.sleep(1.0)

>>> loop = asyncio.get_event_loop()
>>> loop.run_until_complete(hello_world())
>>> loop.close()
Hello World!
Hello World!
...
```

Весь код в asyncio строится на основе понятия цикла обработки событий или, как еще его иногда называют, event loop. event loop — это своего рода планировщик задач или корутин, которые в нем исполняются. Он отвечает за ввод/вывод, он отвечает за управление сигналами, всеми сетевыми операциями и переключает контекст между всеми корутинами, которые в нем зарегистрированы и выполняются. Если одна корутина ожидает завершения какой-то сетевой операции, например, ждет, пока данные поступят в сокет, то в этот момент event loop может переключиться на другую корутину и продолжить ее выполнение. 

При помощи вызова asyncio.get_event_loop мы получаем наш цикл обработки событий. Это объект, и мы можем попросить этот объект исполнить нам некоторую корутину. Опять же, хочу обратить ваше внимание, что обычные функции исполнять нельзя, нужно исполнять именно корутины. Давайте посмотрим, как этот пример работает в консоли. 

**Пример 2**

```python
# asyncio, async def / await; PEP 492 Python3.5

import asyncio


async def hello_world():
    while True:
        print("Hello World!")
        await asyncio.sleep(1.0)

>>> loop = asyncio.get_event_loop()
>>> loop.run_until_complete(hello_world())
>>> loop.close()
Hello World!
Hello World!
```

Начиная с версии Python 3.5, появился новый PEP 492, в котором был введен специальный синтаксис для написания корутин. Это async def и await. Во-первых, этот синтаксис выглядит более лаконично и красиво по сравнению с предыдущим.

**Объявление функции через конструкцию async def гарантирует нам, что эта функция является точно корутиной.**

**Если мы используем этот синтаксис, то внутри мы не можем использовать конструкцию yield from, мы обязаны использовать вызов await.**

**Пример 3. Свой TCP сервер на asyncio**

Cвой TCP сервер, который обрабатывает несколько входящих соединений одновременно.

```python
# asyncio, tcp сервер

import asyncio

async def handle_echo(reader, writer):
    data = await reader.read(1024)
    message = data.decode()
    addr = writer.get_extra_info("peername")
    print("received %r from %r" % (message, addr))
    writer.close()
    
loop = asyncio.get_event_loop()
coro = asyncio.start_server(handle_echo, "127.0.0.1", 10001, loop=loop)
server = loop.run_until_complete(coro)
try:
    loop.run_forever()
except KeyboardInterrupt:
    pass

server.close()
loop.run_until_complete(server.wait_closed())
loop.close()
```

Получаем наш **event_loop**, делаем вызов **start_server**, передаем в этот вызов нашу корутину. В функции **start_server** мы должны еще передать параметры в виде хоста и порта, на котором мы будем слушать наше новое соединение. Далее мы запускаем установку этого соединения и делаем вызов **loop.run_forever**. Тем самым мы будем обрабатывать все входящие соединения, и после того, как мы заакцептили соединение, это все уже будет реализовано внутри кода **asyncio**, для каждого соединения будет создана отдельная корутина, и в этой корутине будет выполнена наша функция. Обратите внимание, какие удобные объекты для того, чтобы работать с нашим сокетом. Есть **reader**. При помощи конструкции **await reader**. Мы можем читать данные из нашего сокета, и также существует **writer**, если нам будет необходимо, мы сможем записывать данные в наш сокет. Давайте посмотрим, как этот пример работает в консоли.

Как это работает в консоли

<img src="/assets/img/2020-12-21-mnogopotochnoe-i-asinhronnoe-programirovanie/8.png">

Сервер готов, чтобы клинеты к нему подключились.

Создадим двух привычных клиентов с помощью модуля **socket**. И отправим на сервер сообщения.

<img src="/assets/img/2020-12-21-mnogopotochnoe-i-asinhronnoe-programirovanie/9.png">

Сервер успешно обработал сообщения от клиентов.

<img src="/assets/img/2020-12-21-mnogopotochnoe-i-asinhronnoe-programirovanie/10.png">

**Наш код работает в одном потоке. Он не захватывает GIL, нет проблемы с GIL.**

Внутри этого кода заложена работа генераторов или корутин. То есть, когда приходит новый запрос, создается новый корутин и эти корутины исполняются последовательно, но, тем самым, мы смогли в одном потоке обработать несколько клиентов. 

**Пример 4. Свой клиент на asyncio**

Рассмотрим код клиента, который тоже может быть асинхронным. Делается это тоже достаточно просто, мы получаем наш **event loop**. 

```python
# asyncio, tcp клиент

import asyncio

async def tcp_echo_client(message, loop):
    reader, writer = await asyncio.open_connection("127.0.0.1", 10004, loop=loop) # чтобы создать соедиение

    print("send: %r" % message)
    writer.write(message.encode())
    writer.close()

loop = asyncio.get_event_loop()
message = "hello World!"
loop.run_until_complete(tcp_echo_client(message, loop))
loop.close()
```

Чтобы создать соединение, мы должны вызвать метод **asyncio.open_connection**. Точно в этот вызов мы должны отправить адресную пару, куда мы делаем соединение, и вызов **await** вернет нам **reader** и **writer**. Это два объекта, при помощи которых можно взаимодействовать с нашим удаленным сервером. То есть, при помощи объекта reader можно читать данные с сервера, при помощи объекта **writer** можно записывать данные на наш сервер. Опять же, вы можете легко создать несколько таких асинхронных клиентов и одновременно выполнять запросы на разные сервера, при этом не делая никаких потоков или процессов. Это очень удобно, просто, и код получается достаточно производительным. Итак, в этом видео мы сделали первые шаги для работы с библиотекой **asyncio**. 




## 3.5. Asyncio. Подробнее

- asyncio.Future
- asyncio.Task
- loop.run_in_executor
- библиотеки для работы с asyncio

```python
### asyncio.Future, аналог concurrent.futures.Future

import asyncio

async def slow_operation(future):
    await asyncio.sleep(1)
    future.set_result("Future is done!")

>>> loop = asyncio.get_event_loop()
>>> future = asyncio.Future()
>>> asyncio.ensure_future(slow_operation(future))
>>> 
>>> loop.run_until_complete(future)
>>> print(future.result())
Future is done!
>>> loop.close()
```

Пример функции, которая называется **slow_operation** — это наша корутина, которую мы объявили. Мы в нее передаем некий объект **future**. **asyncio.Future** — это такой объект, который исполняется, и его выполнение еще не завершено. Этот объект, целиком и полностью, его интерфейс соответствует объекту **concurrent.futures.Future**. Мы разбирали пример работы с этим объектом, когда знакомились с потоками, и исполняли код при помощи **ThreadPoolExecutor** объекта. Давайте разберем, что делается в этом примере. Мы объявили некую функцию, передали в нее наш созданный объект **future**, выполнили **sleep** на одну секунду, и после этого при помощи **set_result** выставили результат в наш объект типа **future**. Обратите внимание - в основной программе мы создаем этот объект, далее мы создаем нашу корутину создаем нашу корутину при помощи **ensure_future**, а в основном цикле обработки событий мы ожидаем завершения нашего объекта **future**, не этой функции, которая называется **slow_operation**, а именно нашего объекта **future**. Таким образом, при помощи объектов класса future можно выстраивать цепочки не только из двух объектов, но и более сложные цепочки, и очень удобно дожидаться завершения выполнения всех объектов. **event loop asyncio** сам исполнит нужный код и вернет нам результаты. 

**Как это работает в консоли**

<img src="/assets/img/2020-12-21-mnogopotochnoe-i-asinhronnoe-programirovanie/11.png">

Как мы видим, наша функция успешно отработала, и мы дождались выполнения нашей созданной **future**, или нашего созданного объекта класса **Future**. 

#### Пример 2. Как можно запустить несколько корутин в одном event loop.

Для этого, как правило, используется объект типа **asyncio.Task**. **asyncio.Task** является наследником класса **asyncio.Future**, и у него существует ряд дополнительных методов. Со всеми этими методами вы можете ознакомиться в документации, но мы рассмотрим лишь базовый принцип того, как создавать таски и как с ними работать. 

```python
### asyncio.Task, запуск нескольких корутин

import asyncio

async def sleep_task(num):
    for i in range(5):
        print(f"process task: {num} iter: {i}")
        await asyncio.sleep(1)
    
    return num

# ensure_future or create_task
>>> loop = asyncio.get_event_loop()

>>> task_list = [loop.create_task(sleep_task(i)) for i in range(2)]
>>> loop.run_until_complete(asyncio.wait(task_list))

>>> loop.run_until_complete(loop.create_task(sleep_task(3)))
>>> loop.run_until_complete(asyncio.gather(
>>>     sleep_task(10),
>>>     sleep_task(20),
>>> ))
```

Напрямую объект типа **asyncio.Task** создавать не нужно. Нужно использовать метод **create_task** из вашего **event loop** и передавать в него корутину. То есть, у каждого объекта типа **Task** есть собственная корутина, которую он внутри исполняет. Итак, мы создаем список из двух тасков. Запоминаем его в виде списка объектов, и далее при помощи метода **asyncio**.**wait** мы исполняем список наших тасков в нашем **event loop**. Обратите внимание, что можно исполнять как список тасков, так и отдельный таск. Например, если мы исполним код, который я сейчас выделил, то не нужно никаких дополнительных функций вида **asyncio.wait**. Также существует более удобная обертка для исполнения списка тасков — это **asyncio.gather**. 

**Как это работает в консоли**

<img src="/assets/img/2020-12-21-mnogopotochnoe-i-asinhronnoe-programirovanie/12.png">

Хочу обратить ваше внимание, что у нас внутри корутины работают последовательно, но все эти корутины исполняются одновременно. Мы видим, что у нас сначала один таск выполняет нулевую итерацию, затем второй таск с номером 1 выполняет свою нулевую итерацию. И затем по очереди нулевой таск выполняет первую итерацию, первый таск выполняет первую итерацию, то есть наши функции, наши корутины, исполняются в event loop'е одновременно, а код при этом мы пишем последовательный. Это очень удобно, код выглядит достаточно просто. Он похож на исполнение потоков, но на самом деле он выполняется последовательно. Давайте еще раз переключимся в консоль и посмотрим на различные методы работы с тасками в **asyncio**.  

Выполняем метод **loop.create_task**, тем самым создавая таск и добавляя его в наш **event loop**, и далее выполняем метод **run_until_complete**. Этот вызов должен нам вернуть результаты выполнения нашего таска.

<img src="/assets/img/2020-12-21-mnogopotochnoe-i-asinhronnoe-programirovanie/13.png">

Да, действительно, мы видим результат выполнения нашего таска — это число 3. 

Попробуем выполнить несколько тасков при помощи удобной функции **asyncio.gather**

Мы запускаем **loop.run_until_complete**, передаем туда удобную функцию **asyncio.gather** и в этой функции перечисляем список тасков. Хочу обратить внимание, в чем отличие. Нам не нужно здесь вызывать метод **create_task**, все внутри будет автоматически вызвано, не нужно запоминать список наших тасков. Вся эта конструкция вернет результаты выполнения. Давайте проверим.

<img src="/assets/img/2020-12-21-mnogopotochnoe-i-asinhronnoe-programirovanie/14.png">

Мы опять видим, что таски хоть и работают последовательно, но запускаются они одновременно и пока один таск выполняет **sleep**, второй продолжает работать и так далее.


##### как исполнить синхронную функцию в event loop


Так как наш **event loop**, или цикл обработки событий, постоянно переключает контекст, и переключает контекст между всеми нашими корутинами и их исполняет последовательно, пока одна корутина ожидает ввода-вывода, вторую корутину наш **event loop** благополучно исполняет. Если код, который будет исполняться в корутине, будет блокирующим, то наш **event loop** не сможет делать переключения контекста, и это будет очень плохо сказываться на вообще всем нашем коде, и с этим нужно что-то делать. Как раз для этого в **asyncio** существует метод **run_in_executor**. Он означает запустить код буквально в пуле потоков, который внутри этого **event loop'а** автоматически будет создан. Можно использовать и собственный пул потоков, а можно использовать уже готовый по умолчанию. Более подробную информацию можно получить в документации, а на этом примере можно наблюдать, как функция urlopen, которая открывает некий url, который ей передали по http, скачивает результаты в отдельном потоке и мы дожидаемся результатов выполнения этой функции, опять же при помощи механизма **event loop**, который называется **futures**. Давайте посмотрим, как он выполняется. 

```python
# loop.run_in_executor, запуск в отдельном потоке

import asyncio
from urllib.request import urlopen

# a synchronous function
def sync_get_url(url):
   return urlopen(url).read()

async def load_url(url, loop=None):
    future = loop.run_in_executor(None, sync_get_url, url)
    response = await future
    print(len(response))

loop = asyncio.get_event_loop()
loop.run_until_complete(load_url("https://google.com", loop=loop))
```

Мы вызываем метод **run_in_executor**. Здесь внутри будет создано нужное количество потоков, и наша функция **sync_get_url** с параметром **url** будет выполнена в отдельном потоке. Для того, чтобы дождаться результата выполнения нашей функции в отдельном потоке, мы используем конструкцию await и передаем туда объект **future**. 


<img src="/assets/img/2020-12-21-mnogopotochnoe-i-asinhronnoe-programirovanie/15.png">

Будем открывать страничку **google.com** и выводить на экран количество байт, которые она занимает. Итак, выполняем наш пример. Мы видим на экране 11 килобайт. Мы исполнили сейчас функцию **urlopen** в отдельном потоке, и в нашем **event loop** мы дождались результатов выполнения этой функции, которая работала синхронно. Как правило, такие задачи будут возникать у вас, если в некоторой библиотеке не будет поддержки работы с **asyncio**. Это может сказаться негативно, как я уже сказал, на производительности, потому что, как мы помним, потоки в Python запускаются и работают с ограничением **GIL**, и будет лучше, если ваш код будет работать без этих вещей. 


















