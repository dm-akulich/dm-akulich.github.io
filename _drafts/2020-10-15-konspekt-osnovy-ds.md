---
layout: post
title: Конспект курса основы Data Science
comments: true
category: ML/DS
tags: python ds
---

<meta name="robots" content="noindex" />

**7 первых строк датафрэйма df**

```python
df.iloc[0:7]
df.loc[:6] # если индекс представляет собой последовательность чисел от 0 с шагом 1
df.head(7)
df.iloc[:7]
```

**Базовые понятия**

```python
df.describe() # Описательные статистики;
df.dtypes # Типы полей. int64 - числовая непрерывная, object - грубо говоря, номинативная;
df.shape # Сколько строчек и колонок;
df.groupby('gender').aggregate({'writing score' : 'mean'}) # группировка и агрегирование;
df.iloc[0:5, 0:5] # обращение к строчкам и столбцам IntegerLocation
df.iloc[[0, 3, 10], 0:5]
perf_with_names = df.iloc[[0, 3, 4, 7, 8]]
perf_with_names.index = ["Cersei", "Tywin", "Gregor", "Joffrey", "Ilyn Payne"]
students_performance.iloc[:, 0] # все строки в первой колонке
```

### 1.4 Сделаем DF из Series

```python
my_series1 = pd.Series([1, 2, 3], index=["Cersei", "Tywin", "Gregor"])
my_series2 = pd.Series([4, 5, 6], index=["Cersei", "Tywin", "Gregor"])
pd.DataFrame({'col_name_1': my_series1, 'col_name_2': my_series2})
```

### 1.5 Фильтрация данных

```python
df.loc[df.gender == 'female'].head() # фильтрация п о значению колонки
df.loc[df.gender == 'female', ['gender', 'writing score']].head() # выведем отдельные колонки по фильтру со строками
```

Отбререм наблюдения, где значение выше среднего

```python
mean_writing_score = df['writing score'].mean()
df.loc[df['writing score'] > mean_writing_score, ['gender', 'writing score']].head()
```

*Два условия для фильтрации*

```python
# Вариант 1 
df[(df['gender']=='female') & (df['writing score']>10)]

# Вариант 2
query = (df['gender']=='female') & (df['writing score']>10)
df.loc[query]
```

```python
df1.var() # дисперсия
```

**Задача 1**: *Как различается среднее и дисперсия оценок по предметам у групп студентов со стандартным или урезанным ланчем?*

```python
df.groupby('lunch').describe().unstack(1) 
```


**Познакомимся с методом ```query```**

```python
df = df.rename(columns={'parental level of education': 'parental_level_of_education', # Переименуем колонки
                    'test preparation course': 'test_preparation_course',
                    'math score': 'math_score',
                    'reading score': 'reading_score',
                    'writing score': 'writing_score'})

# вариант 1
df.query('math_score > 50')

# вариант 2
custom_query = 75
df.query("math_score > @custom_query & lunch == 'standard' ")
```

query не применяется к колонкам, название которых содержит недопустимые символы (типа пробел, слэша). Ещё пример ```query```, аналогичный ```isin()``` - ```variants = ['var1', 'var2']```

```python
df.query('col1 == @variants')
```

Отбор отдельно взятых колонок, у которых есть "score" в названии колонки.

```python
k = [i for i in list(df) if 'score' in i] # получим список со значениями, содержащими score в имени
df[k]
```

```
    math_score	reading_score	writing_score
0	72	        72	            74
1	69	        90	            88
2	90	        95	            93
```

Можно также использовать метод ```filter```

```python
df.filter(like="score", axis=1) # axis=1 - это столбики по умолчанию, axis=0 - строчки
```

**Про метод ```filter```**

- Аргумент regex отбирает лэйблы, подходящие к переданному паттерну
- Аргумент axis отвечает за то, какие лэйблы мы рассматриваем - индекс или колонки
- Аргумент like отбирает те лэйблы, где встречается поданное значение
- Можно использовать только один из аргументов items, like, regex
- Аргумент items отбирает лэйблы, переданные в коллекции в качестве значения

### 1.6 Группировка и агрегация

*Как использовать ```groupby``` в связке с ```aggregate```*

Самый простой пример группировки 

```python
df.groupby('gender').mean()

	    math_score	reading_score	writing_score
gender			
female	63.633205	72.608108	72.467181
male	68.728216	65.473029	63.311203
```

Мы можем более подробно расписать, какие функции нам нужны. Вместо ```mean()``` используем ```aggregate({'col_name_1':'func_name_1', 'col_name_2':'func_name_2'})```.

```python
df.groupby('gender', as_index=False)\
    .aggregate({'math_score': 'mean', 'reading_score': 'mean', 'writing_score': 'mean'})\
    .rename(columns = {'math_score': 'mean_math_score', 'reading_score':'mean_reading_score', 'writing_score': 'mean_writing_score'})
```

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/1.png">

Мы можем группировать переменные сразу по нескольким переменным. Например, по ```gender``` и ```race/ethnicity```

```python
# Двойную группировку обернули в скобки
df.groupby(['gender','race/ethnicity'], as_index=False)\ 
    .aggregate({'math_score': 'mean', 'reading_score': 'mean', 'writing_score': 'mean'})\
    .rename(columns = {'math_score': 'mean_math_score', 'reading_score':'mean_reading_score', 'writing_score': 'mean_writing_score'})
```

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/2.png">

**Создание новых колонок**

Сделаем результирующую успеваемость.

```python
# Вариант 1
df['total_score'] = df.math_score + df.reading_score + df.writing_score
df.head()

# Вариант 2. Метод assign. Чтобы применить функцию для всех элементов
df = df.assign(total_score_log = np.log(df.total_score))
df.head()
```

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/3.png">

Удаление колонок. Удалим ```total_score```.

```python
df = df.drop(['total_score'], axis=1)
```

### 1.7 Визуализация, matplotlib, seaborn

*Полезные ссылки*:

<ul>
	<li><a href="https://matplotlib.org/" rel="noopener noreferrer nofollow" target="_blank">matplotlib</a></li>
	<li><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html" rel="noopener noreferrer nofollow" target="_blank">встроенная в pandas визуализация</a></li>
	<li><a href="https://seaborn.pydata.org/" rel="noopener noreferrer nofollow" target="_blank">seaborn</a></li>
</ul>


Самый просто способ - использовать встроенный метод.

```python
import pandas as pd
import numpy as np
%matplotlib inline
import matplotlib as plt
import seaborn as sns
df = pd.read_csv('/.../df.csv')
df['reading score'].hist()
```

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/4.png">

Нарисуем ```scatterplot``` с помощью ```matplotlib```.

```python
df.plot.scatter(x='math score', y='reading score')
```

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/5.png">

Теперь сделаем через ```seaborn```.

```python
ax = sns.lmplot(x='math score', y='reading score', hue='gender', data=df, fit_reg=False)
ax.set_xlabels("Math Score")
ax.set_ylabels("Reading Score")
```

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/6.png">

**Задача 1**: *Вам дан датасэт с 2-мя фичами (колонками). Постройте график распределения точек (наблюдений) в пространстве этих 2-ух переменных (одна из них будет x, а другая - y) и напишите число кластеров, формируемых наблюдениями.*

```python
df.head()
sns.lmplot(x='x', y='y', data=df)
```

**Задача 2**:*Скачайте данные, представляющие геномные расстояния между видами, и постройте тепловую карту, чтобы различия было видно наглядно. В ответ впишите, какая картинка соответствует скачанным данным. Чтобы график отображался как на картинках, добавьте*

```python
g = # ваш код для создания теплокарты, укажите параметр cmap="viridis" для той же цветовой схемы
g.xaxis.set_ticks_position('top')
g.xaxis.set_tick_params(rotation=90)
```

```python
df = pd.read_csv('.../df.csv', index_col=0)
g = sns.heatmap(df, cmap="viridis")
g.xaxis.set_ticks_position('top')
g.xaxis.set_tick_params(rotation=90)
```

- [Мануал по рисованию violin плотов](https://seaborn.pydata.org/generated/seaborn.violinplot.html)
- [Мануал по рисованию Pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html)

**Задача 3**:*Продолжаем изучение ирисов! Ещё один важный тип графиков - pairplot, отражающий зависимость пар переменных друг от друга, а также распределение каждой из переменных. Постройте его и посмотрите на scatter плоты для каждой из пар фичей. Какая из пар навскидку имеет наибольшую корреляцию?*

```python
sns.pairplot(iris, kind="hist")
```

**Задача 4**: *Любым удобным для вас способом создайте dataframe c именем my_data, в котором две колонки c именами (type - строки, value - целые числа) и четыре наблюдения в каждой колонке:*

```python
type_data = pd.Series(['A', 'A', 'B', 'B'], dtype='object')
value_data = pd.Series([10, 14, 12, 23], dtype='int64')
my_data = pd.DataFrame({'type': type_data, 'value': value_data})
```

**Задача 5**: *Начнем с простого, в dataframe с именем my_stat сохранено 20 строк и четыре колонки (V1, V2, V3, V4): В переменную с именем subset_1 сохраните только первые 10 строк и только 1 и 3 колонку. В переменную с именем subset_2 сохраните все строки кроме 1 и 5 и только 2 и 4 колонку. Помните, что нумерация индексов строк и колонок начинается с 0.*

```python
subset_1=my_stat[['V1', 'V3']].head(10)
d1 = my_stat[~my_stat.index.isin([0,4])]
subset_2=d1[['V2', 'V4']]
```

**Задача 6**: *Теперь потренируемся отбирать нужные нам наблюдения (строки), соответствующие некому условию. В dataframe с именем my_stat четыре колонки V1, V2, V3, V4: В переменную subset_1 сохраните только те наблюдения, у которых значения переменной V1 строго больше 0, и значение переменной V3  равняется 'A'. В переменную subset_2  сохраните только те наблюдения, у которых значения переменной V2  не равняются 10, или значения переменной V4 больше или равно 1.*

```python
subset_1 = my_stat[(my_stat['V1'] > 0) & (my_stat['V3']=='A')]
my_stat[(my_stat['V2'] != 10) | (my_stat['V4'] >= 1)]
```

**Задача 7**: *Теперь давайте преобразуем наши данные. В переменной my_stat лежат данные с которыми вам необходимо проделать следующее дейтвие. В этих данных (my_stat) создайте две новые переменных: V5 = V1 + V4; V6 = натуральный логарифм переменной V2*

```python
my_stat['V5'] = my_stat.V1 + my_stat.V4
my_stat = my_stat.assign(V6 = np.log(my_stat.V2))
```

**Задача 8**: *Переменные V1, V2  ... такие имена никуда не годятся. С такими названиями легко запутаться в собственных данных и в результате ошибиться в расчетах. Переименуйте колонки в данных  my_stat следующим образом: V1 -> session_value; V2 -> group; V3 -> time; V4 -> n_users*

```python
my_stat = my_stat.rename(columns={'V1': 'session_value', 'V2': 'group', 'V3': 'time', 'V4': 'n_users'})
```

**Задача 9**: *Как заменять наблюдения в данных. В dataframe с именем my_stat сохранены данные с 4 колонками: session_value, group, time, n_users. В переменной session_value замените все пропущенные значения на нули. В переменной n_users замените все отрицательные значения на медианное значение переменной n_users (без учета отрицательных значений, разумеется). Как получить медиану переменной n_users без учета отрицательных значений? Если не придумали решения, подсказка - ищите один из возможных ответов в задачах, которые мы уже решили.*

```python
my_stat['session_value'] = my_stat['session_value'].fillna(0)
median = my_stat['n_users'][my_stat['n_users'] >= 0].median()
my_stat['n_users'] = np.where(my_stat['n_users'] < 0, median, my_stat['n_users'])
```

**Задача 10**: *Для данных my_stat рассчитайте среднее значение переменной session_value для каждой группы (переменная group), в получившемся dataframe  переменная group не должна превратиться в индекс. Также переименуйте колонку со средним значением session_value в mean_session_value.*

```python
mean_session_value_data = my_stat.groupby('group', as_index=False).\
    aggregate({'session_value': 'mean'}).\
    rename(columns={'session_value': 'mean_session_value'})
```



# 2. Решающие деревья. Method Decision Tree

Как он работает, как он обучается, почему это круче, чем линейная регрессия. Будет немного библиотеки scikit-learn.

Поймем, что значит:

- Как убедиться, что модель обучилась
- Переобучилась
- Разберем различные метрики(accuracy - общая точность, precition, recall, )

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/7.png">

Самый просто скрипт на Python выглядел бы как набор ef/else'ов.

Не совсем понятно, какие именно фичи решающее дерево использует дял классификации, именно выбирает нужны порог. 

Разбереемся с тем, как это все работает. 

***Ремарка о том, как получить представление данных в виде точек в пространстве.***

*Представьте, что у вас есть маленький датафрэйм с 2-мя характеристиками цветов и их видом (который мы потом будем предсказывать, то есть это целевая переменная).*

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/8.png">

*Теперь просто возьмём каждое наблюдение из датасэта и поместим его в пространство, где будет 2 оси, соответствующие фичам - petal length и petal width. 1 точка в пространстве - 1 наблюдение из датафрэйма, у которого положение в пространстве определяется значениями petal length и petal width. Получится следующая картина (цвета соответствуют виду цветка)*

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/9.png">

*Такая репрезентация бывает очень удобной для представления работы моделей. Точно так же строится и в случае большего числа фичей - просто увеличивается число осей и пространство становится многомерным. В случае определения являются ли письма спамом или нет, исходные данные представляют собой текст. Его так просто не запихнёшь в большинство моделей и для представления датасэта как множества точек нужны предварительные преобразования. Как вариант - посчитать число букв и число восклицательных знаков в каждом письме и уже на основании этих данных произвести визуализацию.*

Перед тем как поймем принцип работы решающих деревьев. Разберемся, зачем они вообще нужны, если мы уже разбирали линенейную и логистическую регресии ? У линейной и логистической регрессий, несмотря на то, что они легко обучаемы, есть ряд недостатков, счвязанных  с тем, что данные должны быть линейно раззделимы, строгие требования к переменным и тп. В ситуациях, когда задачи более сложные, чем линейные зависимости. Линейные модели не очень хороши. Хорошо бы научиться использовать такие методы, которые свободны от тех ограничений, которые есть у линейной регрессии. Было бы здорово, использовать те методы, у которых этих ограничений изначально нет, например, когда мы говорим о классификации двух классов, у методов решающих деревьев в принципе нет такого ограничения, что было бы круто, чтобы данные были линейно разделимы. Или, если бы речь шла о регрессии, то чтобы остатки были распределены обязательно нормально. Эти ограничения не столь важны.

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/10.png">

Если для логистической регрессии было бы проблемой разделить данные на слайде выше, то для других методов ML это вообще не проблема. При этом у других методов: нейронных сетей, дешающих деревьев есть свои преимущества, но и есть свои недостатки. И утвержнение, что дрешающие деревья всегда лучше логистической регресии не всегда справедливо. 

Сделаем простой игрушенчый пример Desicion Tree. И разберем его.

```python
from sklearn import tree
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

data = pd.DataFrame({'X_1': [1, 1, 1, 0, 0, 0, 0, 1], 'X_2': [0, 0, 0, 1, 0, 0, 0, 1], 'Y': [1, 1, 1, 1, 0, 0, 0, 0]})
data
```

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/11.png">

У нас есть небольшой df и наша задача в том, чтобы научиться при помощи дерева решений предсказать значение ```Y```, основываясь на значениях ```X_1``` и ```X_2```.

Посмотрим, как дерево решений справится с этой задачей.


```python
clf = tree.DecisionTreeClassifier(criterion='entropy') # Делаем наш классифаер. Дерево решений это тоже объект, у которого есть свои методы, атрибуты и тп. 
# наш классификатор, это объект, который может обучиться, предсказать и др. 
```

Сделаем небольшую предобработку данных, а именно сохраним все фичи в переменную ```X```. А целевую ```Y``` сохраним в ```y```.

```python
X = data[['X_1', 'X_2']]
y = data.Y
```

Чтобы обучить модель, используем метод ```fit``` у классифаера.

```python
clf.fit(X, y)
```

Самый простой способ понять, чему научилось дерево, это его визуализировать.

```python
tree.plot_tree(clf.fit(X, y))
```

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/12.png">

В самом начале было 4 наблюдения с классом 0, 4 с классом 1 <- ```value=[4,4]```. Нужно узнать что за энтропия.



## Теория и энтропия

Разберем, что за математическая идея лежит за обучением дерева. Как оно понимает какой должно быть глубины, какие епременные оно должно использовать, что по ним сплититься и по каким значениям. 

Разберем на игрушечном примере. 

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/12.png">

За счет знания каких-либо фич мы максимально снижаем неопределенность. Допустим, если пол мужской, то ...., если возраст больше 30, то ....

Введем термин **энтропии** - энтропия, это уровень неопределенности наших данных. Энтропия тем выше, чем хуже нам удается разделять классы. 

$$ E =  -\sum_{i=1}^{n}p_{i}*log_{2}(p_{i}) $$

Что на практике означает формула. У нас есть некоторое состояние, когда у нас 50/50 положительных и отрицательных элементов нашего класса. Чему тогда ровняется энтропия в нашем случае ?

$$ E =  -\frac{1}{2}*log_{2}(\frac{1}{2})-\frac{1}{2}*log_{2}(\frac{1}{2}) = \frac{1}{2} + \frac{1}{2} = 1 $$

1 - максимальная степень неопределенности.

Запишем формулу энтропии для случая, когда в данных **все примеры отрицательные**.

$$ E =  -1 * log_{2}(1) - 0 * log_{2}(0) = 0 $$

Что будет, если у нас **все примеры положительные**?

$$ E = 0 * log_{2}(0) -1 * log_{2}(1)  = 0 $$

Грарфик для энтропии $$ \frac{1}{2} $$ выглядит следующим образом.

**В чем заключается основная идея?** Сначала мы фиксируем исходное состояние дел: выписываем сколько представителей положительного класса и отрицательного. Считаем, чему равняется энтропия в этой текущей ситуации. Если 50/50, то $$ E = 1 $$ максимальная неопределнность. Дальше смотрим на фичи. Что будет если возьмем значение фичи $$ X_{2} $$

**Основные идеи дерева решений и энтропии**

- [Материал для повторения алгоритма обучения дерева](https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8)
- [Подробная лекция про решающие деревья](https://www.youtube.com/watch?v=-dCtJjlEEgM), мне очень понравилось, как излагается материал, настоятельно рекомендую вместе/до/после/ нашего курса.
- [Очень крутая визуализация принципа работы решающих деревьев!](http://www.r2d3.us/Наглядное-Введение-в-Теорию-Машинного-Обучения/)


**Задача 1**: *посчитаем энтропию, чтобы лучше понять, формализуемость разделения на группы.*

```python
data = pd.DataFrame()
data["X1"] = [1,1,1,1,1,1,1,1,1,0] # Шерстит
data["X2"] = [1,1,1,1,0,0,0,0,1,0] # Гавкает
data["X3"] = [0,0,0,0,1,1,1,1,1,1] # Лазает по деревьям
data["Y"] = [1,1,1,1, 0,0,0,0,0,0] # 0 - собачка, 1 - котик
```

```python
E_sh_sob=round((1/1)*m.log2((1/1)) - 0, 2)
E_sh_kot=round(-(4/9)*m.log2((4/9)) - (5/9)*m.log2((5/9)), 2)
E_gav_sob=round(0 - (5/5)*m.log2((5/5)), 2)
E_gav_kot=round(-(4/5)*m.log2((4/5)) - (1/5)*m.log2((1/5)), 2)
E_laz_sob=round(0 - (6/6)*m.log2((6/6)), 2)
E_laz_kot=-(4/4)*m.log2((4/4)) - 0
```

Энтропия при разделении по фиче Шерстист в группах, где Шерстист равно 0 и 1 соответственно, составляет **0** и **0.99** . Энтропия при разделении по фиче Гавкает в группах, где Гавкает равно 0 и 1 соответственно, составляет **0** и **0.72**. Энтропия при разделении по фиче Лазает по деревьям в группах, где эта фича равна 0 и 1 соответственно, составляет **0** и **0** .

**Задача 2**: *посчитаем Information Gain по данным из предыдущего задания. Впишите через пробел округлённые до 2-ого знака значения IG для фичей Шерстист, Гавкает и Лазает по деревьям. Десятичным разделителем в данном задании является точка.*

```python
E = -(4/10)*m.log2(4/10) - (6/10)*m.log2(6/10)

IG_sh = E - (1/10)*E_sh_sob - (9/10)*E_sh_kot
IG_gav = E - (5/10)*E_gav_sob - (5/10)*E_gav_kot
IG_laz = E - (6/10)*E_laz_sob - (6/10)*E_laz_kot

print(round(IG_sh, 2), round(IG_gav,2), round(IG_laz,2))
```

**Заключение по деревьям решения и энтропии**: когда tree пытатся разделить наблюдения на два класса, происходит следующее:
- изначально фиксируется статус-кво. Например, есть 8 точек, 4 из них относятся к классу A, 4 относятся к классу В. Мы находимся в ситуации полной неопределенности. 
- далее выбирается такая фича и такой сплит по этой фиче, который позволяет привнести в ситуацию **как можно бОльшее снижение энтропии**. Как можно больше **Information Gain**, чтобы на этом этапе уже была не рандомная классификация, а какая-то более перекошеная к одному из классов. 
- дальше, когда мы сделали один сплит и оказались в двух разных ситуациях, повторяем то же самое. Там все еще присутствует некоторая неопределенность. Ее тоже нужно снизить, она не 50/50, а допустим 25/75. Снова выбирается такая фича и сплит по фиче, чтобы эту неопределенность снизить. И пока мы не отнесли все точки к нужному классу, не снизили энтропию до нуля. Будем продолжать сплититься по фичам, опускаться нижи и ниже.  

### Потренеруемся на датасете Титаника делать Desicion Tree

- ```train.csv``` - данные с Игреком;
- ```test.csv``` - данные без Игрека, только фичи.

*Импортируем нужные библиотеки.*

```python
from sklearn import tree
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
#
from IPython.display import SVG
from graphviz import Source
from IPython.display import display
# 
from IPython.display import HTML
style = "<style>svg{width:70% !important; height:70% !important;}</style>"
```

*Считаем данные titanic data*

```python
titanic_data = pd.read_csv('/Users/dima/Documents/learn/stepik_osnovy_data_science/titanic_datasets/train.csv')
titanic_data.head()
```

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/13.png">

Задача сейчас: обучить дерево решений так, чтобы предсказывать, выжил ли человек или нет, основываясь на тех данных, которые у нас есть.

Сделаем небольшую предобработку, посмотрим пропущенные значения (ну если там 99% пропущенные - вполне выкидываем).

Для каждой колонки посомтрим, сколько в ней пропущенных значений.

```python
titanic_data.isnull().sum()
```

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/14.png">

Отбросим ```PassengerId```, ```Cabin```, ```Ticket```, ```Name```. И посохраняем переменные.

```python
X = titanic_data.drop(['PassengerId', 'Survived', 'Name', 'Ticket', 'Cabin'], axis=1)
y = titanic_data.Survived
```

Так как ```tree``` не умеет работать с строковыми данными. Закодируем их с помощью get_dummies. Также Дерево Решений не умеет работать с пропущенными значениями. Заполним переменную возраст просто медианным значением.

```python
X = titanic_data.drop(['PassengerId', 'Survived', 'Name', 'Ticket', 'Cabin'], axis=1)
X = pd.get_dummies(X) # кодируем
X = X.fillna({'Age': X.Age.median()}) # Заполнили нули на медианное
y = titanic_data.Survived
X.head()
```

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/15.png">

А теперь обучим дерево.

```python
clf = tree.DecisionTreeClassifier(criterion='entropy')
clf.fit(X, y)
tree.plot_tree(clf)
```

Данное дерево не решает задачи по сути. Оно решает задачу классификации конкретно на этих пассажирах. Это переобученная модель, которая никакого паттерна не имеет. Оно пытается классифицировать каждое наблюдение. 

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/16.png">

В дереве решений нет необходимости до последнего сплитить. Важно общий паттерн найти. Если не задать ему параметров, то оно будет сплитить до полследнего. Модель получится ни о чем.

Чтобы как-то ограничить дерево, есть две идеи: 
- 1. Ограничить дерево в потребности расти как можно глубже.
- 2. Показывать дереву два набора данных (для обучения, для тестирования).

Для этого потребуется метод ```train_test_split```. Он разбивает DF на множества для обучения и для тестов.

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=42) #  random_state это параметр генератора случайных чисел, который позволяет при каждом вызове разбивать выборку одинаковым образом.
print(X_train.shape, X_test.shape)
```

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/17.png">

Вернемся к самой задаче: убедимся, что проблема с деревом, которое получилось действительно есть.

Проверим, как хорошо модель предсказывает методом ```score```.

```python
clf.score(X, y)
>>> 1.0 # Мы не ошиблись нигде
# Теперь возмем и обучим модель только на данных для обучения и проверим ее точность
clf.fit(X_train, y_train)
clf.score(X_train, y_train)
>>> 0.9161073825503355
# Теперь закинем данные для проверки
clf.score(X_test, y_test)
>>> 0.7966101694915254
```

Так как на обучающих данных успех 91%, а на тестовых всего 80% , то дерево у нас переобучено.

Самый простой способ это исправить - ограничить глубину дерева. Ограничим глубину до 5 и обучим заново.

```python
clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=5)
clf.fit(X_train, y_train)
clf.score(X_train, y_train)
>>> 0.8406040268456376
clf.score(X_test, y_test)
>>> 0.8067796610169492
```

**Результат на тестых данных улучшился!!**

В scikit-lern есть методы, позволяющие подобирать параметры наиболее эффективно. Но мы пока ручками поработаем. 

### Обучение, переобучение, недообучение и кросвалидация

Заведем массив параметров, который будем варьировать: max_depth. Ну и визуализируем.

```python
max_depth_values = range(1, 100)
scores_data = pd.DataFrame()
for max_depth in max_depth_values:
    clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=max_depth)
    clf.fit(X_train, y_train)
    train_score = clf.score(X_train, y_train)
    test_score = clf.score(X_test, y_test)
    
    temp_score_data = pd.DataFrame({'max_depth': [max_depth],
                                    'train_score': [train_score],
                                    'test_score': [test_score]})

    scores_data = scores_data.append(temp_score_data)

scores_data_long =pd.melt(scores_data, id_vars=['max_depth'], value_vars=['train_score', 'test_score'],
                          var_name='set_type', value_name='score')

sns.lineplot(x="max_depth", y='score', hue='set_type', data=scores_data_long)
```

<img src="/assets/img/2020-10-15-konspekt-osnovy-ds/18.png">

Изучим этот чудесный график.

С увеличение глубины дерева все лучше и лучше классифицируем сет классифицируемых данных. С тест_скором сначала растет, а потом начинает падать, на этом месте глубину лучше и оставить, чтобы не переобучить (глубина от 3-х до 5) - оптимальное состояние.

```python

```



```python

```




